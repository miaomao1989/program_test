

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>31. Vhost Sample Application &mdash; Data Plane Development Kit 16.04.0 documentation</title>
  

  
  

  

  
  
    

  

  
  
    <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  

  

  
    <link rel="top" title="Data Plane Development Kit 16.04.0 documentation" href="../index.html"/>
        <link rel="up" title="Sample Applications User Guide" href="index.html"/>
        <link rel="next" title="32. Netmap Compatibility Sample Application" href="netmap_compatibility.html"/>
        <link rel="prev" title="30. VMDQ and DCB Forwarding Sample Application" href="vmdq_dcb_forwarding.html"/> 

  
  <script src="../_static/js/modernizr.min.js"></script>

</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
          

          
            <a href="../index.html" class="icon icon-home"> Data Plane Development Kit
          

          
            
            <img src="../_static/DPDK_logo_vertical_rev_small.png" class="logo" />
          
          </a>

          
            
            
              <div class="version">
                16.04.0
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
                <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../linux_gsg/index.html">Getting Started Guide for Linux</a></li>
<li class="toctree-l1"><a class="reference internal" href="../freebsd_gsg/index.html">Getting Started Guide for FreeBSD</a></li>
<li class="toctree-l1"><a class="reference internal" href="../xen/index.html">Xen Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../prog_guide/index.html">Programmer&#8217;s Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../nics/index.html">Network Interface Controller Drivers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cryptodevs/index.html">Crypto Device Drivers</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="index.html">Sample Applications User Guide</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="intro.html">1. Introduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="cmd_line.html">2. Command Line Sample Application</a></li>
<li class="toctree-l2"><a class="reference internal" href="ethtool.html">3. Ethtool Sample Application</a></li>
<li class="toctree-l2"><a class="reference internal" href="exception_path.html">4. Exception Path Sample Application</a></li>
<li class="toctree-l2"><a class="reference internal" href="hello_world.html">5. Hello World Sample Application</a></li>
<li class="toctree-l2"><a class="reference internal" href="skeleton.html">6. Basic Forwarding Sample Application</a></li>
<li class="toctree-l2"><a class="reference internal" href="rxtx_callbacks.html">7. RX/TX Callbacks Sample Application</a></li>
<li class="toctree-l2"><a class="reference internal" href="ip_frag.html">8. IP Fragmentation Sample Application</a></li>
<li class="toctree-l2"><a class="reference internal" href="ipv4_multicast.html">9. IPv4 Multicast Sample Application</a></li>
<li class="toctree-l2"><a class="reference internal" href="ip_reassembly.html">10. IP Reassembly Sample Application</a></li>
<li class="toctree-l2"><a class="reference internal" href="kernel_nic_interface.html">11. Kernel NIC Interface Sample Application</a></li>
<li class="toctree-l2"><a class="reference internal" href="keep_alive.html">12. Keep Alive Sample Application</a></li>
<li class="toctree-l2"><a class="reference internal" href="l2_forward_crypto.html">13. L2 Forwarding with Crypto Sample Application</a></li>
<li class="toctree-l2"><a class="reference internal" href="l2_forward_job_stats.html">14. L2 Forwarding Sample Application (in Real and Virtualized Environments) with core load statistics.</a></li>
<li class="toctree-l2"><a class="reference internal" href="l2_forward_real_virtual.html">15. L2 Forwarding Sample Application (in Real and Virtualized Environments)</a></li>
<li class="toctree-l2"><a class="reference internal" href="l2_forward_cat.html">16. L2 Forwarding Sample Application with Cache Allocation Technology (CAT)</a></li>
<li class="toctree-l2"><a class="reference internal" href="l3_forward.html">17. L3 Forwarding Sample Application</a></li>
<li class="toctree-l2"><a class="reference internal" href="l3_forward_power_man.html">18. L3 Forwarding with Power Management Sample Application</a></li>
<li class="toctree-l2"><a class="reference internal" href="l3_forward_access_ctrl.html">19. L3 Forwarding with Access Control Sample Application</a></li>
<li class="toctree-l2"><a class="reference internal" href="l3_forward_virtual.html">20. L3 Forwarding in a Virtualization Environment Sample Application</a></li>
<li class="toctree-l2"><a class="reference internal" href="link_status_intr.html">21. Link Status Interrupt Sample Application</a></li>
<li class="toctree-l2"><a class="reference internal" href="load_balancer.html">22. Load Balancer Sample Application</a></li>
<li class="toctree-l2"><a class="reference internal" href="multi_process.html">23. Multi-process Sample Application</a></li>
<li class="toctree-l2"><a class="reference internal" href="qos_metering.html">24. QoS Metering Sample Application</a></li>
<li class="toctree-l2"><a class="reference internal" href="qos_scheduler.html">25. QoS Scheduler Sample Application</a></li>
<li class="toctree-l2"><a class="reference internal" href="intel_quickassist.html">26. IntelÂ® QuickAssist Technology Sample Application</a></li>
<li class="toctree-l2"><a class="reference internal" href="quota_watermark.html">27. Quota and Watermark Sample Application</a></li>
<li class="toctree-l2"><a class="reference internal" href="timer.html">28. Timer Sample Application</a></li>
<li class="toctree-l2"><a class="reference internal" href="packet_ordering.html">29. Packet Ordering Application</a></li>
<li class="toctree-l2"><a class="reference internal" href="vmdq_dcb_forwarding.html">30. VMDQ and DCB Forwarding Sample Application</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">31. Vhost Sample Application</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#background">31.1. Background</a></li>
<li class="toctree-l3"><a class="reference internal" href="#sample-code-overview">31.2. Sample Code Overview</a></li>
<li class="toctree-l3"><a class="reference internal" href="#supported-distributions">31.3. Supported Distributions</a></li>
<li class="toctree-l3"><a class="reference internal" href="#prerequisites">31.4. Prerequisites</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#installing-packages-on-the-host-vhost-cuse-required">31.4.1. Installing Packages on the Host(vhost cuse required)</a></li>
<li class="toctree-l4"><a class="reference internal" href="#qemu-simulator">31.4.2. QEMU simulator</a></li>
<li class="toctree-l4"><a class="reference internal" href="#setting-up-the-execution-environment">31.4.3. Setting up the Execution Environment</a></li>
<li class="toctree-l4"><a class="reference internal" href="#setting-up-the-guest-execution-environment">31.4.4. Setting up the Guest Execution Environment</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#compiling-the-sample-code">31.5. Compiling the Sample Code</a></li>
<li class="toctree-l3"><a class="reference internal" href="#running-the-sample-code">31.6. Running the Sample Code</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#parameters">31.6.1. Parameters</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#running-the-virtual-machine-qemu">31.7. Running the Virtual Machine (QEMU)</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#redirecting-qemu-to-vhost-net-sample-code-vhost-cuse">31.7.1. Redirecting QEMU to vhost-net Sample Code(vhost cuse)</a></li>
<li class="toctree-l4"><a class="reference internal" href="#mapping-the-virtual-machine-s-memory">31.7.2. Mapping the Virtual Machine&#8217;s Memory</a></li>
<li class="toctree-l4"><a class="reference internal" href="#qemu-wrapper-script">31.7.3. QEMU Wrapper Script</a></li>
<li class="toctree-l4"><a class="reference internal" href="#libvirt-integration">31.7.4. Libvirt Integration</a></li>
<li class="toctree-l4"><a class="reference internal" href="#common-issues">31.7.5. Common Issues</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#running-dpdk-in-the-virtual-machine">31.8. Running DPDK in the Virtual Machine</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#testpmd-mac-forwarding">31.8.1. Testpmd MAC Forwarding</a></li>
<li class="toctree-l4"><a class="reference internal" href="#running-testpmd">31.8.2. Running Testpmd</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#passing-traffic-to-the-virtual-machine-device">31.9. Passing Traffic to the Virtual Machine Device</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="netmap_compatibility.html">32. Netmap Compatibility Sample Application</a></li>
<li class="toctree-l2"><a class="reference internal" href="ip_pipeline.html">33. Internet Protocol (IP) Pipeline Application</a></li>
<li class="toctree-l2"><a class="reference internal" href="test_pipeline.html">34. Test Pipeline Application</a></li>
<li class="toctree-l2"><a class="reference internal" href="dist_app.html">35. Distributor Sample Application</a></li>
<li class="toctree-l2"><a class="reference internal" href="vm_power_management.html">36. VM Power Management Application</a></li>
<li class="toctree-l2"><a class="reference internal" href="tep_termination.html">37. TEP termination Sample Application</a></li>
<li class="toctree-l2"><a class="reference internal" href="proc_info.html">38. dpdk_proc_info Application</a></li>
<li class="toctree-l2"><a class="reference internal" href="ptpclient.html">39. PTP Client Sample Application</a></li>
<li class="toctree-l2"><a class="reference internal" href="performance_thread.html">40. Performance Thread Sample Application</a></li>
<li class="toctree-l2"><a class="reference internal" href="ipsec_secgw.html">41. IPsec Security Gateway Sample Application</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../testpmd_app_ug/index.html">Testpmd Application User Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/index.html">FAQ</a></li>
<li class="toctree-l1"><a class="reference internal" href="../rel_notes/index.html">Release Notes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../contributing/index.html">Contributor&#8217;s Guidelines</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
        <a href="../index.html">Data Plane Development Kit</a>
      </nav>


      
      <div class="wy-nav-content">
        <div class="rst-content">
          

 



<div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="../index.html">Docs</a> &raquo;</li>
      
          <li><a href="index.html">Sample Applications User Guide</a> &raquo;</li>
      
    <li>31. Vhost Sample Application</li>
      <li class="wy-breadcrumbs-aside">
        
          
            <a href="../_sources/sample_app_ug/vhost.txt" rel="nofollow"> View page source</a>
          
        
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="vhost-sample-application">
<h1>31. Vhost Sample Application</h1>
<p>The vhost sample application demonstrates integration of the Data Plane Development Kit (DPDK)
with the Linux* KVM hypervisor by implementing the vhost-net offload API.
The sample application performs simple packet switching between virtual machines based on Media Access Control
(MAC) address or Virtual Local Area Network (VLAN) tag.
The splitting of Ethernet traffic from an external switch is performed in hardware by the Virtual Machine Device Queues
(VMDQ) and Data Center Bridging (DCB) features of the IntelÂ® 82599 10 Gigabit Ethernet Controller.</p>
<div class="section" id="background">
<h2>31.1. Background</h2>
<p>Virtio networking (virtio-net) was developed as the Linux* KVM para-virtualized method for communicating network packets
between host and guest.
It was found that virtio-net performance was poor due to context switching and packet copying between host, guest, and QEMU.
The following figure shows the system architecture for a virtio-based networking (virtio-net).</p>
<div class="figure" id="id1">
<span id="figure-qemu-virtio-net"></span><img alt="../_images/qemu_virtio_net.png" src="../_images/qemu_virtio_net.png" />
<p class="caption"><span class="caption-number">Fig. 31.1 </span><span class="caption-text">System Architecture for Virtio-based Networking (virtio-net).</span></p>
</div>
<p>The Linux* Kernel vhost-net module was developed as an offload mechanism for virtio-net.
The vhost-net module enables KVM (QEMU) to offload the servicing of virtio-net devices to the vhost-net kernel module,
reducing the context switching and packet copies in the virtual dataplane.</p>
<p>This is achieved by QEMU sharing the following information with the vhost-net module through the vhost-net API:</p>
<ul class="simple">
<li>The layout of the guest memory space, to enable the vhost-net module to translate addresses.</li>
<li>The locations of virtual queues in QEMU virtual address space,
to enable the vhost module to read/write directly to and from the virtqueues.</li>
<li>An event file descriptor (eventfd) configured in KVM to send interrupts to the virtio- net device driver in the guest.
This enables the vhost-net module to notify (call) the guest.</li>
<li>An eventfd configured in KVM to be triggered on writes to the virtio-net device&#8217;s
Peripheral Component Interconnect (PCI) config space.
This enables the vhost-net module to receive notifications (kicks) from the guest.</li>
</ul>
<p>The following figure shows the system architecture for virtio-net networking with vhost-net offload.</p>
<div class="figure" id="id2">
<span id="figure-virtio-linux-vhost"></span><img alt="../_images/virtio_linux_vhost.png" src="../_images/virtio_linux_vhost.png" />
<p class="caption"><span class="caption-number">Fig. 31.2 </span><span class="caption-text">Virtio with Linux</span></p>
</div>
</div>
<div class="section" id="sample-code-overview">
<h2>31.2. Sample Code Overview</h2>
<p>The DPDK vhost-net sample code demonstrates KVM (QEMU) offloading the servicing of a Virtual Machine&#8217;s (VM&#8217;s)
virtio-net devices to a DPDK-based application in place of the kernel&#8217;s vhost-net module.</p>
<p>The DPDK vhost-net sample code is based on vhost library. Vhost library is developed for user space Ethernet switch to
easily integrate with vhost functionality.</p>
<p>The vhost library implements the following features:</p>
<ul class="simple">
<li>Management of virtio-net device creation/destruction events.</li>
<li>Mapping of the VM&#8217;s physical memory into the DPDK vhost-net&#8217;s address space.</li>
<li>Triggering/receiving notifications to/from VMs via eventfds.</li>
<li>A virtio-net back-end implementation providing a subset of virtio-net features.</li>
</ul>
<p>There are two vhost implementations in vhost library, vhost cuse and vhost user. In vhost cuse, a character device driver is implemented to
receive and process vhost requests through ioctl messages. In vhost user, a socket server is created to received vhost requests through
socket messages. Most of the messages share the same handler routine.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last"><strong>Any vhost cuse specific requirement in the following sections will be emphasized</strong>.</p>
</div>
<p>Two implementations are turned on and off statically through configure file. Only one implementation could be turned on. They don&#8217;t co-exist in current implementation.</p>
<p>The vhost sample code application is a simple packet switching application with the following feature:</p>
<ul class="simple">
<li>Packet switching between virtio-net devices and the network interface card,
including using VMDQs to reduce the switching that needs to be performed in software.</li>
</ul>
<p>The following figure shows the architecture of the Vhost sample application based on vhost-cuse.</p>
<div class="figure" id="id3">
<span id="figure-vhost-net-arch"></span><img alt="../_images/vhost_net_arch1.png" src="../_images/vhost_net_arch1.png" />
<p class="caption"><span class="caption-number">Fig. 31.3 </span><span class="caption-text">Vhost-net Architectural Overview</span></p>
</div>
<p>The following figure shows the flow of packets through the vhost-net sample application.</p>
<div class="figure" id="id4">
<span id="figure-vhost-net-sample-app"></span><img alt="../_images/vhost_net_sample_app.png" src="../_images/vhost_net_sample_app.png" />
<p class="caption"><span class="caption-number">Fig. 31.4 </span><span class="caption-text">Packet Flow Through the vhost-net Sample Application</span></p>
</div>
</div>
<div class="section" id="supported-distributions">
<h2>31.3. Supported Distributions</h2>
<p>The example in this section have been validated with the following distributions:</p>
<ul class="simple">
<li>Fedora* 18</li>
<li>Fedora* 19</li>
<li>Fedora* 20</li>
</ul>
</div>
<div class="section" id="prerequisites">
<span id="vhost-app-prerequisites"></span><h2>31.4. Prerequisites</h2>
<p>This section lists prerequisite packages that must be installed.</p>
<div class="section" id="installing-packages-on-the-host-vhost-cuse-required">
<h3>31.4.1. Installing Packages on the Host(vhost cuse required)</h3>
<p>The vhost cuse code uses the following packages; fuse, fuse-devel, and kernel-modules-extra.
The vhost user code don&#8217;t rely on those modules as eventfds are already installed into vhost process through
Unix domain socket.</p>
<ol class="arabic">
<li><p class="first">Install Fuse Development Libraries and headers:</p>
<div class="highlight-console"><div class="highlight"><pre><span></span><span class="go">yum -y install fuse fuse-devel</span>
</pre></div>
</div>
</li>
<li><p class="first">Install the Cuse Kernel Module:</p>
<div class="highlight-console"><div class="highlight"><pre><span></span><span class="go">yum -y install kernel-modules-extra</span>
</pre></div>
</div>
</li>
</ol>
</div>
<div class="section" id="qemu-simulator">
<h3>31.4.2. QEMU simulator</h3>
<p>For vhost user, qemu 2.2 is required.</p>
</div>
<div class="section" id="setting-up-the-execution-environment">
<h3>31.4.3. Setting up the Execution Environment</h3>
<p>The vhost sample code requires that QEMU allocates a VM&#8217;s memory on the hugetlbfs file system.
As the vhost sample code requires hugepages,
the best practice is to partition the system into separate hugepage mount points for the VMs and the vhost sample code.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">This is best-practice only and is not mandatory.
For systems that only support 2 MB page sizes,
both QEMU and vhost sample code can use the same hugetlbfs mount point without issue.</p>
</div>
<p><strong>QEMU</strong></p>
<p>VMs with gigabytes of memory can benefit from having QEMU allocate their memory from 1 GB huge pages.
1 GB huge pages must be allocated at boot time by passing kernel parameters through the grub boot loader.</p>
<ol class="arabic">
<li><p class="first">Calculate the maximum memory usage of all VMs to be run on the system.
Then, round this value up to the nearest Gigabyte the execution environment will require.</p>
</li>
<li><p class="first">Edit the /etc/default/grub file, and add the following to the GRUB_CMDLINE_LINUX entry:</p>
<div class="highlight-console"><div class="highlight"><pre><span></span><span class="go">GRUB_CMDLINE_LINUX=&quot;... hugepagesz=1G hugepages=&lt;Number of hugepages required&gt; default_hugepagesz=1G&quot;</span>
</pre></div>
</div>
</li>
<li><p class="first">Update the grub boot loader:</p>
<div class="highlight-console"><div class="highlight"><pre><span></span><span class="go">grub2-mkconfig -o /boot/grub2/grub.cfg</span>
</pre></div>
</div>
</li>
<li><p class="first">Reboot the system.</p>
</li>
<li><p class="first">The hugetlbfs mount point (/dev/hugepages) should now default to allocating gigabyte pages.</p>
</li>
</ol>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Making the above modification will change the system default hugepage size to 1 GB for all applications.</p>
</div>
<p><strong>Vhost Sample Code</strong></p>
<p>In this section, we create a second hugetlbs mount point to allocate hugepages for the DPDK vhost sample code.</p>
<ol class="arabic">
<li><p class="first">Allocate sufficient 2 MB pages for the DPDK vhost sample code:</p>
<div class="highlight-console"><div class="highlight"><pre><span></span><span class="go">echo 256 &gt; /sys/kernel/mm/hugepages/hugepages-2048kB/nr_hugepages</span>
</pre></div>
</div>
</li>
<li><p class="first">Mount hugetlbs at a separate mount point for 2 MB pages:</p>
<div class="highlight-console"><div class="highlight"><pre><span></span><span class="go">mount -t hugetlbfs nodev /mnt/huge -o pagesize=2M</span>
</pre></div>
</div>
</li>
</ol>
<p>The above steps can be automated by doing the following:</p>
<ol class="arabic">
<li><p class="first">Edit /etc/fstab to add an entry to automatically mount the second hugetlbfs mount point:</p>
<div class="highlight-none"><div class="highlight"><pre><span></span>hugetlbfs &lt;tab&gt; /mnt/huge &lt;tab&gt; hugetlbfs defaults,pagesize=1G 0 0
</pre></div>
</div>
</li>
<li><p class="first">Edit the /etc/default/grub file, and add the following to the GRUB_CMDLINE_LINUX entry:</p>
<div class="highlight-none"><div class="highlight"><pre><span></span>GRUB_CMDLINE_LINUX=&quot;... hugepagesz=2M hugepages=256 ... default_hugepagesz=1G&quot;
</pre></div>
</div>
</li>
<li><p class="first">Update the grub bootloader:</p>
<div class="highlight-console"><div class="highlight"><pre><span></span><span class="go">grub2-mkconfig -o /boot/grub2/grub.cfg</span>
</pre></div>
</div>
</li>
<li><p class="first">Reboot the system.</p>
</li>
</ol>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Ensure that the default hugepage size after this setup is 1 GB.</p>
</div>
</div>
<div class="section" id="setting-up-the-guest-execution-environment">
<h3>31.4.4. Setting up the Guest Execution Environment</h3>
<p>It is recommended for testing purposes that the DPDK testpmd sample application is used in the guest to forward packets,
the reasons for this are discussed in <a class="reference internal" href="#running-the-virtual-machine-qemu">Running the Virtual Machine (QEMU)</a>.</p>
<p>The testpmd application forwards packets between pairs of Ethernet devices,
it requires an even number of Ethernet devices (virtio or otherwise) to execute.
It is therefore recommended to create multiples of two virtio-net devices for each Virtual Machine either through libvirt or
at the command line as follows.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Observe that in the example, &#8220;-device&#8221; and &#8220;-netdev&#8221; are repeated for two virtio-net devices.</p>
</div>
<p>For vhost cuse:</p>
<div class="highlight-console"><div class="highlight"><pre><span></span><span class="go">qemu-system-x86_64 ... \</span>
<span class="go">-netdev tap,id=hostnet1,vhost=on,vhostfd=&lt;open fd&gt; \</span>
<span class="go">-device virtio-net-pci, netdev=hostnet1,id=net1 \</span>
<span class="go">-netdev tap,id=hostnet2,vhost=on,vhostfd=&lt;open fd&gt; \</span>
<span class="go">-device virtio-net-pci, netdev=hostnet2,id=net1</span>
</pre></div>
</div>
<p>For vhost user:</p>
<div class="highlight-console"><div class="highlight"><pre><span></span><span class="go">qemu-system-x86_64 ... \</span>
<span class="go">-chardev socket,id=char1,path=&lt;sock_path&gt; \</span>
<span class="go">-netdev type=vhost-user,id=hostnet1,chardev=char1 \</span>
<span class="go">-device virtio-net-pci,netdev=hostnet1,id=net1 \</span>
<span class="go">-chardev socket,id=char2,path=&lt;sock_path&gt; \</span>
<span class="go">-netdev type=vhost-user,id=hostnet2,chardev=char2 \</span>
<span class="go">-device virtio-net-pci,netdev=hostnet2,id=net2</span>
</pre></div>
</div>
<p>sock_path is the path for the socket file created by vhost.</p>
</div>
</div>
<div class="section" id="compiling-the-sample-code">
<h2>31.5. Compiling the Sample Code</h2>
<ol class="arabic">
<li><p class="first">Compile vhost lib:</p>
<p>To enable vhost, turn on vhost library in the configure file config/common_linuxapp.</p>
<div class="highlight-console"><div class="highlight"><pre><span></span><span class="go">CONFIG_RTE_LIBRTE_VHOST=n</span>
</pre></div>
</div>
<p>vhost user is turned on by default in the configure file config/common_linuxapp.
To enable vhost cuse, disable vhost user.</p>
<div class="highlight-console"><div class="highlight"><pre><span></span><span class="go">   CONFIG_RTE_LIBRTE_VHOST_USER=y</span>

<span class="go">After vhost is enabled and the implementation is selected, build the vhost library.</span>
</pre></div>
</div>
</li>
<li><p class="first">Go to the examples directory:</p>
<div class="highlight-console"><div class="highlight"><pre><span></span><span class="go">export RTE_SDK=/path/to/rte_sdk</span>
<span class="go">cd ${RTE_SDK}/examples/vhost</span>
</pre></div>
</div>
</li>
<li><p class="first">Set the target (a default target is used if not specified). For example:</p>
<div class="highlight-console"><div class="highlight"><pre><span></span><span class="go">export RTE_TARGET=x86_64-native-linuxapp-gcc</span>
</pre></div>
</div>
<p>See the DPDK Getting Started Guide for possible RTE_TARGET values.</p>
</li>
<li><p class="first">Build the application:</p>
<div class="highlight-console"><div class="highlight"><pre><span></span><span class="go">cd ${RTE_SDK}</span>
<span class="go">make config ${RTE_TARGET}</span>
<span class="go">make install ${RTE_TARGET}</span>
<span class="go">cd ${RTE_SDK}/examples/vhost</span>
<span class="go">make</span>
</pre></div>
</div>
</li>
<li><p class="first">Go to the eventfd_link directory(vhost cuse required):</p>
<div class="highlight-console"><div class="highlight"><pre><span></span><span class="go">cd ${RTE_SDK}/lib/librte_vhost/eventfd_link</span>
</pre></div>
</div>
</li>
<li><p class="first">Build the eventfd_link kernel module(vhost cuse required):</p>
<div class="highlight-console"><div class="highlight"><pre><span></span><span class="go">make</span>
</pre></div>
</div>
</li>
</ol>
</div>
<div class="section" id="running-the-sample-code">
<h2>31.6. Running the Sample Code</h2>
<ol class="arabic">
<li><p class="first">Install the cuse kernel module(vhost cuse required):</p>
<div class="highlight-console"><div class="highlight"><pre><span></span><span class="go">modprobe cuse</span>
</pre></div>
</div>
</li>
<li><p class="first">Go to the eventfd_link directory(vhost cuse required):</p>
<div class="highlight-console"><div class="highlight"><pre><span></span><span class="go">export RTE_SDK=/path/to/rte_sdk</span>
<span class="go">cd ${RTE_SDK}/lib/librte_vhost/eventfd_link</span>
</pre></div>
</div>
</li>
<li><p class="first">Install the eventfd_link module(vhost cuse required):</p>
<div class="highlight-console"><div class="highlight"><pre><span></span><span class="go">insmod ./eventfd_link.ko</span>
</pre></div>
</div>
</li>
<li><p class="first">Go to the examples directory:</p>
<div class="highlight-console"><div class="highlight"><pre><span></span><span class="go">export RTE_SDK=/path/to/rte_sdk</span>
<span class="go">cd ${RTE_SDK}/examples/vhost/build/app</span>
</pre></div>
</div>
</li>
<li><p class="first">Run the vhost-switch sample code:</p>
<p>vhost cuse:</p>
<div class="highlight-console"><div class="highlight"><pre><span></span><span class="go">./vhost-switch -c f -n 4 --socket-mem 1024 --huge-dir /mnt/huge \</span>
<span class="go"> -- -p 0x1 --dev-basename usvhost</span>
</pre></div>
</div>
<p>vhost user: a socket file named usvhost will be created under current directory. Use its path as the socket path in guest&#8217;s qemu commandline.</p>
<div class="highlight-console"><div class="highlight"><pre><span></span><span class="go">./vhost-switch -c f -n 4 --socket-mem 1024 --huge-dir /mnt/huge \</span>
<span class="go"> -- -p 0x1 --dev-basename usvhost</span>
</pre></div>
</div>
</li>
</ol>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Please note the huge-dir parameter instructs the DPDK to allocate its memory from the 2 MB page hugetlbfs.</p>
</div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">The number used with the &#8211;socket-mem parameter may need to be more than 1024.
The number required depends on the number of mbufs allocated by vhost-switch.</p>
</div>
<div class="section" id="parameters">
<span id="vhost-app-parameters"></span><h3>31.6.1. Parameters</h3>
<p><strong>Basename.</strong>
vhost cuse uses a Linux* character device to communicate with QEMU.
The basename is used to generate the character devices name.</p>
<blockquote>
<div>/dev/&lt;basename&gt;</div></blockquote>
<p>For compatibility with the QEMU wrapper script, a base name of &#8220;usvhost&#8221; should be used:</p>
<div class="highlight-console"><div class="highlight"><pre><span></span><span class="go">./vhost-switch -c f -n 4 --socket-mem 1024 --huge-dir /mnt/huge \</span>
<span class="go"> -- -p 0x1 --dev-basename usvhost</span>
</pre></div>
</div>
<p><strong>vm2vm.</strong>
The vm2vm parameter disable/set mode of packet switching between guests in the host.
Value of &#8220;0&#8221; means disabling vm2vm implies that on virtual machine packet transmission will always go to the Ethernet port;
Value of &#8220;1&#8221; means software mode packet forwarding between guests, it needs packets copy in vHOST,
so valid only in one-copy implementation, and invalid for zero copy implementation;
value of &#8220;2&#8221; means hardware mode packet forwarding between guests, it allows packets go to the Ethernet port,
hardware L2 switch will determine which guest the packet should forward to or need send to external,
which bases on the packet destination MAC address and VLAN tag.</p>
<div class="highlight-console"><div class="highlight"><pre><span></span><span class="go">./vhost-switch -c f -n 4 --socket-mem 1024 --huge-dir /mnt/huge \</span>
<span class="go"> -- --vm2vm [0,1,2]</span>
</pre></div>
</div>
<p><strong>Mergeable Buffers.</strong>
The mergeable buffers parameter controls how virtio-net descriptors are used for virtio-net headers.
In a disabled state, one virtio-net header is used per packet buffer;
in an enabled state one virtio-net header is used for multiple packets.
The default value is 0 or disabled since recent kernels virtio-net drivers show performance degradation with this feature is enabled.</p>
<div class="highlight-console"><div class="highlight"><pre><span></span><span class="go">./vhost-switch -c f -n 4 --socket-mem 1024 --huge-dir /mnt/huge \</span>
<span class="go"> -- --mergeable [0,1]</span>
</pre></div>
</div>
<p><strong>Stats.</strong>
The stats parameter controls the printing of virtio-net device statistics.
The parameter specifies an interval second to print statistics, with an interval of 0 seconds disabling statistics.</p>
<div class="highlight-console"><div class="highlight"><pre><span></span><span class="go">./vhost-switch -c f -n 4 --socket-mem 1024 --huge-dir /mnt/huge \</span>
<span class="go">-- --stats [0,n]</span>
</pre></div>
</div>
<p><strong>RX Retry.</strong>
The rx-retry option enables/disables enqueue retries when the guests RX queue is full.
This feature resolves a packet loss that is observed at high data-rates,
by allowing it to delay and retry in the receive path.
This option is enabled by default.</p>
<div class="highlight-console"><div class="highlight"><pre><span></span><span class="go">./vhost-switch -c f -n 4 --socket-mem 1024 --huge-dir /mnt/huge \</span>
<span class="go"> -- --rx-retry [0,1]</span>
</pre></div>
</div>
<p><strong>RX Retry Number.</strong>
The rx-retry-num option specifies the number of retries on an RX burst,
it takes effect only when rx retry is enabled.
The default value is 4.</p>
<div class="highlight-console"><div class="highlight"><pre><span></span><span class="go">./vhost-switch -c f -n 4 --socket-mem 1024 --huge-dir /mnt/huge \</span>
<span class="go"> -- --rx-retry 1 --rx-retry-num 5</span>
</pre></div>
</div>
<p><strong>RX Retry Delay Time.</strong>
The rx-retry-delay option specifies the timeout (in micro seconds) between retries on an RX burst,
it takes effect only when rx retry is enabled.
The default value is 15.</p>
<div class="highlight-console"><div class="highlight"><pre><span></span><span class="go">./vhost-switch -c f -n 4 --socket-mem 1024 --huge-dir /mnt/huge \</span>
<span class="go"> -- --rx-retry 1 --rx-retry-delay 20</span>
</pre></div>
</div>
<p><strong>Zero copy.</strong>
The zero copy option enables/disables the zero copy mode for RX/TX packet,
in the zero copy mode the packet buffer address from guest translate into host physical address
and then set directly as DMA address.
If the zero copy mode is disabled, then one copy mode is utilized in the sample.
This option is disabled by default.</p>
<div class="highlight-console"><div class="highlight"><pre><span></span><span class="go">./vhost-switch -c f -n 4 --socket-mem 1024 --huge-dir /mnt/huge \</span>
<span class="go"> -- --zero-copy [0,1]</span>
</pre></div>
</div>
<p><strong>RX descriptor number.</strong>
The RX descriptor number option specify the Ethernet RX descriptor number,
Linux legacy virtio-net has different behavior in how to use the vring descriptor from DPDK based virtio-net PMD,
the former likely allocate half for virtio header, another half for frame buffer,
while the latter allocate all for frame buffer,
this lead to different number for available frame buffer in vring,
and then lead to different Ethernet RX descriptor number could be used in zero copy mode.
So it is valid only in zero copy mode is enabled. The value is 32 by default.</p>
<div class="highlight-console"><div class="highlight"><pre><span></span><span class="go">./vhost-switch -c f -n 4 --socket-mem 1024 --huge-dir /mnt/huge \</span>
<span class="go"> -- --zero-copy 1 --rx-desc-num [0, n]</span>
</pre></div>
</div>
<p><strong>TX descriptor number.</strong>
The TX descriptor number option specify the Ethernet TX descriptor number, it is valid only in zero copy mode is enabled.
The value is 64 by default.</p>
<div class="highlight-console"><div class="highlight"><pre><span></span><span class="go">./vhost-switch -c f -n 4 --socket-mem 1024 --huge-dir /mnt/huge \</span>
<span class="go"> -- --zero-copy 1 --tx-desc-num [0, n]</span>
</pre></div>
</div>
<p><strong>VLAN strip.</strong>
The VLAN strip option enable/disable the VLAN strip on host, if disabled, the guest will receive the packets with VLAN tag.
It is enabled by default.</p>
<div class="highlight-console"><div class="highlight"><pre><span></span><span class="go">./vhost-switch -c f -n 4 --socket-mem 1024 --huge-dir /mnt/huge \</span>
<span class="go"> -- --vlan-strip [0, 1]</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="running-the-virtual-machine-qemu">
<span id="vhost-app-running"></span><h2>31.7. Running the Virtual Machine (QEMU)</h2>
<p>QEMU must be executed with specific parameters to:</p>
<ul>
<li><p class="first">Ensure the guest is configured to use virtio-net network adapters.</p>
<div class="highlight-console"><div class="highlight"><pre><span></span><span class="go">qemu-system-x86_64 ... -device virtio-net-pci,netdev=hostnet1, \</span>
<span class="go">id=net1 ...</span>
</pre></div>
</div>
</li>
<li><p class="first">Ensure the guest&#8217;s virtio-net network adapter is configured with offloads disabled.</p>
<div class="highlight-console"><div class="highlight"><pre><span></span><span class="go">qemu-system-x86_64 ... -device virtio-net-pci,netdev=hostnet1, \</span>
<span class="go">id=net1, csum=off,gso=off,guest_tso4=off,guest_tso6=off,guest_ecn=off</span>
</pre></div>
</div>
</li>
<li><p class="first">Redirect QEMU to communicate with the DPDK vhost-net sample code in place of the vhost-net kernel module(vhost cuse).</p>
<div class="highlight-console"><div class="highlight"><pre><span></span><span class="go">qemu-system-x86_64 ... -netdev tap,id=hostnet1,vhost=on, \</span>
<span class="go">vhostfd=&lt;open fd&gt; ...</span>
</pre></div>
</div>
</li>
<li><p class="first">Enable the vhost-net sample code to map the VM&#8217;s memory into its own process address space.</p>
<div class="highlight-console"><div class="highlight"><pre><span></span><span class="go">qemu-system-x86_64 ... -mem-prealloc -mem-path /dev/hugepages ...</span>
</pre></div>
</div>
</li>
</ul>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">The QEMU wrapper (qemu-wrap.py) is a Python script designed to automate the QEMU configuration described above.
It also facilitates integration with libvirt, although the script may also be used standalone without libvirt.</p>
</div>
<div class="section" id="redirecting-qemu-to-vhost-net-sample-code-vhost-cuse">
<h3>31.7.1. Redirecting QEMU to vhost-net Sample Code(vhost cuse)</h3>
<p>To redirect QEMU to the vhost-net sample code implementation of the vhost-net API,
an open file descriptor must be passed to QEMU running as a child process.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="ch">#!/usr/bin/python</span>
<span class="n">fd</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="s2">&quot;/dev/usvhost-1&quot;</span><span class="p">,</span> <span class="n">os</span><span class="o">.</span><span class="n">O_RDWR</span><span class="p">)</span>
<span class="n">subprocess</span><span class="o">.</span><span class="n">call</span>
<span class="p">(</span><span class="s2">&quot;qemu-system-x86_64 ... -netdev tap,id=vhostnet0,vhost=on,vhostfd=&quot;</span>
  <span class="o">+</span> <span class="n">fd</span> <span class="o">+</span><span class="s2">&quot;...&quot;</span><span class="p">,</span> <span class="n">shell</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</pre></div>
</div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">This process is automated in the <a class="reference internal" href="#qemu-wrapper-script">QEMU Wrapper Script</a>.</p>
</div>
</div>
<div class="section" id="mapping-the-virtual-machine-s-memory">
<h3>31.7.2. Mapping the Virtual Machine&#8217;s Memory</h3>
<p>For the DPDK vhost-net sample code to be run correctly, QEMU must allocate the VM&#8217;s memory on hugetlbfs.
This is done by specifying mem-prealloc and mem-path when executing QEMU.
The vhost-net sample code accesses the virtio-net device&#8217;s virtual rings and packet buffers
by finding and mapping the VM&#8217;s physical memory on hugetlbfs.
In this case, the path passed to the guest should be that of the 1 GB page hugetlbfs:</p>
<div class="highlight-console"><div class="highlight"><pre><span></span><span class="go">qemu-system-x86_64 ... -mem-prealloc -mem-path /dev/hugepages ...</span>
</pre></div>
</div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">This process is automated in the <a class="reference internal" href="#qemu-wrapper-script">QEMU Wrapper Script</a>.
The following two sections only applies to vhost cuse.
For vhost-user, please make corresponding changes to qemu-wrapper script and guest XML file.</p>
</div>
</div>
<div class="section" id="qemu-wrapper-script">
<h3>31.7.3. QEMU Wrapper Script</h3>
<p>The QEMU wrapper script automatically detects and calls QEMU with the necessary parameters required
to integrate with the vhost sample code.
It performs the following actions:</p>
<ul class="simple">
<li>Automatically detects the location of the hugetlbfs and inserts this into the command line parameters.</li>
<li>Automatically open file descriptors for each virtio-net device and inserts this into the command line parameters.</li>
<li>Disables offloads on each virtio-net device.</li>
<li>Calls Qemu passing both the command line parameters passed to the script itself and those it has auto-detected.</li>
</ul>
<p>The QEMU wrapper script will automatically configure calls to QEMU:</p>
<div class="highlight-console"><div class="highlight"><pre><span></span><span class="go">qemu-wrap.py -machine pc-i440fx-1.4,accel=kvm,usb=off \</span>
<span class="go">-cpu SandyBridge -smp 4,sockets=4,cores=1,threads=1 \</span>
<span class="go">-netdev tap,id=hostnet1,vhost=on \</span>
<span class="go">-device virtio-net-pci,netdev=hostnet1,id=net1 \</span>
<span class="go">-hda &lt;disk img&gt; -m 4096</span>
</pre></div>
</div>
<p>which will become the following call to QEMU:</p>
<div class="highlight-console"><div class="highlight"><pre><span></span><span class="go">qemu-system-x86_64 -machine pc-i440fx-1.4,accel=kvm,usb=off \</span>
<span class="go">-cpu SandyBridge -smp 4,sockets=4,cores=1,threads=1 \</span>
<span class="go">-netdev tap,id=hostnet1,vhost=on,vhostfd=&lt;open fd&gt; \</span>
<span class="go">-device virtio-net-pci,netdev=hostnet1,id=net1, \</span>
<span class="go">csum=off,gso=off,guest_tso4=off,guest_tso6=off,guest_ecn=off \</span>
<span class="go">-hda &lt;disk img&gt; -m 4096 -mem-path /dev/hugepages -mem-prealloc</span>
</pre></div>
</div>
</div>
<div class="section" id="libvirt-integration">
<h3>31.7.4. Libvirt Integration</h3>
<p>The QEMU wrapper script (qemu-wrap.py) &#8220;wraps&#8221; libvirt calls to QEMU,
such that QEMU is called with the correct parameters described above.
To call the QEMU wrapper automatically from libvirt, the following configuration changes must be made:</p>
<ul>
<li><p class="first">Place the QEMU wrapper script in libvirt&#8217;s binary search PATH ($PATH).
A good location is in the directory that contains the QEMU binary.</p>
</li>
<li><p class="first">Ensure that the script has the same owner/group and file permissions as the QEMU binary.</p>
</li>
<li><p class="first">Update the VM xml file using virsh edit &lt;vm name&gt;:</p>
<ul>
<li><p class="first">Set the VM to use the launch script</p>
</li>
<li><p class="first">Set the emulator path contained in the #&lt;emulator&gt;&lt;emulator/&gt; tags For example,
replace &lt;emulator&gt;/usr/bin/qemu-kvm&lt;emulator/&gt; with  &lt;emulator&gt;/usr/bin/qemu-wrap.py&lt;emulator/&gt;</p>
</li>
<li><p class="first">Set the VM&#8217;s virtio-net device&#8217;s to use vhost-net offload:</p>
<div class="highlight-xml"><div class="highlight"><pre><span></span><span class="nt">&lt;interface</span> <span class="na">type=</span><span class="s">&quot;network&quot;</span><span class="nt">&gt;</span>
<span class="nt">&lt;model</span> <span class="na">type=</span><span class="s">&quot;virtio&quot;</span><span class="nt">/&gt;</span>
<span class="nt">&lt;driver</span> <span class="na">name=</span><span class="s">&quot;vhost&quot;</span><span class="nt">/&gt;</span>
<span class="nt">&lt;interface/&gt;</span>
</pre></div>
</div>
</li>
<li><p class="first">Enable libvirt to access the DPDK Vhost sample code&#8217;s character device file by adding it
to controllers cgroup for libvirtd using the following steps:</p>
<div class="highlight-xml"><div class="highlight"><pre><span></span>cgroup_controllers = [ ... &quot;devices&quot;, ... ] clear_emulator_capabilities = 0
user = &quot;root&quot; group = &quot;root&quot;
cgroup_device_acl = [
    &quot;/dev/null&quot;, &quot;/dev/full&quot;, &quot;/dev/zero&quot;,
    &quot;/dev/random&quot;, &quot;/dev/urandom&quot;,
    &quot;/dev/ptmx&quot;, &quot;/dev/kvm&quot;, &quot;/dev/kqemu&quot;,
    &quot;/dev/rtc&quot;, &quot;/dev/hpet&quot;, &quot;/dev/net/tun&quot;,
    &quot;/dev/<span class="nt">&lt;devbase-name&gt;</span>-<span class="nt">&lt;index&gt;</span>&quot;,
]
</pre></div>
</div>
</li>
</ul>
</li>
<li><p class="first">Disable SELinux  or set to permissive mode.</p>
</li>
<li><p class="first">Mount cgroup device controller:</p>
<div class="highlight-console"><div class="highlight"><pre><span></span><span class="go">mkdir /dev/cgroup</span>
<span class="go">mount -t cgroup none /dev/cgroup -o devices</span>
</pre></div>
</div>
</li>
<li><p class="first">Restart the libvirtd system process</p>
<p>For example, on Fedora* &#8220;systemctl restart libvirtd.service&#8221;</p>
</li>
<li><p class="first">Edit the configuration parameters section of the script:</p>
<ul>
<li><p class="first">Configure the &#8220;emul_path&#8221; variable to point to the QEMU emulator.</p>
<div class="highlight-xml"><div class="highlight"><pre><span></span>emul_path = &quot;/usr/local/bin/qemu-system-x86_64&quot;
</pre></div>
</div>
</li>
<li><p class="first">Configure the &#8220;us_vhost_path&#8221; variable to point to the DPDK vhost-net sample code&#8217;s character devices name.
DPDK vhost-net sample code&#8217;s character device will be in the format &#8220;/dev/&lt;basename&gt;&#8221;.</p>
<div class="highlight-xml"><div class="highlight"><pre><span></span>us_vhost_path = &quot;/dev/usvhost&quot;
</pre></div>
</div>
</li>
</ul>
</li>
</ul>
</div>
<div class="section" id="common-issues">
<h3>31.7.5. Common Issues</h3>
<ul>
<li><p class="first">QEMU failing to allocate memory on hugetlbfs, with an error like the following:</p>
<div class="highlight-none"><div class="highlight"><pre><span></span>file_ram_alloc: can&#39;t mmap RAM pages: Cannot allocate memory
</pre></div>
</div>
<p>When running QEMU the above error indicates that it has failed to allocate memory for the Virtual Machine on
the hugetlbfs. This is typically due to insufficient hugepages being free to support the allocation request.
The number of free hugepages can be checked as follows:</p>
<div class="highlight-console"><div class="highlight"><pre><span></span><span class="go">cat /sys/kernel/mm/hugepages/hugepages-&lt;pagesize&gt;/nr_hugepages</span>
</pre></div>
</div>
<p>The command above indicates how many hugepages are free to support QEMU&#8217;s allocation request.</p>
</li>
<li><p class="first">User space VHOST when the guest has 2MB sized huge pages:</p>
<p>The guest may have 2MB or 1GB sized huge pages. The user space VHOST should work properly in both cases.</p>
</li>
<li><p class="first">User space VHOST will not work with QEMU without the <code class="docutils literal"><span class="pre">-mem-prealloc</span></code> option:</p>
<p>The current implementation works properly only when the guest memory is pre-allocated, so it is required to
use a QEMU version (e.g. 1.6) which supports <code class="docutils literal"><span class="pre">-mem-prealloc</span></code>. The <code class="docutils literal"><span class="pre">-mem-prealloc</span></code> option must be
specified explicitly in the QEMU command line.</p>
</li>
<li><p class="first">User space VHOST will not work with a QEMU version without shared memory mapping:</p>
<p>As shared memory mapping is mandatory for user space VHOST to work properly with the guest, user space VHOST
needs access to the shared memory from the guest to receive and transmit packets. It is important to make sure
the QEMU version supports shared memory mapping.</p>
</li>
<li><p class="first">In an Ubuntu environment, QEMU fails to start a new guest normally with user space VHOST due to not being able
to allocate huge pages for the new guest:</p>
<p>The solution for this issue is to add <code class="docutils literal"><span class="pre">-boot</span> <span class="pre">c</span></code> into the QEMU command line to make sure the huge pages are
allocated properly and then the guest should start normally.</p>
<p>Use <code class="docutils literal"><span class="pre">cat</span> <span class="pre">/proc/meminfo</span></code> to check if there is any changes in the value of <code class="docutils literal"><span class="pre">HugePages_Total</span></code> and <code class="docutils literal"><span class="pre">HugePages_Free</span></code>
after the guest startup.</p>
</li>
<li><p class="first">Log message: <code class="docutils literal"><span class="pre">eventfd_link:</span> <span class="pre">module</span> <span class="pre">verification</span> <span class="pre">failed:</span> <span class="pre">signature</span> <span class="pre">and/or</span> <span class="pre">required</span> <span class="pre">key</span> <span class="pre">missing</span> <span class="pre">-</span> <span class="pre">tainting</span> <span class="pre">kernel</span></code>:</p>
<p>This log message may be ignored. The message occurs due to the kernel module <code class="docutils literal"><span class="pre">eventfd_link</span></code>, which is not a standard
Linux module but which is necessary for the user space VHOST current implementation (CUSE-based) to communicate with
the guest.</p>
</li>
</ul>
</div>
</div>
<div class="section" id="running-dpdk-in-the-virtual-machine">
<span id="vhost-app-running-dpdk"></span><h2>31.8. Running DPDK in the Virtual Machine</h2>
<p>For the DPDK vhost-net sample code to switch packets into the VM,
the sample code must first learn the MAC address of the VM&#8217;s virtio-net device.
The sample code detects the address from packets being transmitted from the VM, similar to a learning switch.</p>
<p>This behavior requires no special action or configuration with the Linux* virtio-net driver in the VM
as the Linux* Kernel will automatically transmit packets during device initialization.
However, DPDK-based applications must be modified to automatically transmit packets during initialization
to facilitate the DPDK vhost- net sample code&#8217;s MAC learning.</p>
<p>The DPDK testpmd application can be configured to automatically transmit packets during initialization
and to act as an L2 forwarding switch.</p>
<div class="section" id="testpmd-mac-forwarding">
<h3>31.8.1. Testpmd MAC Forwarding</h3>
<p>At high packet rates, a minor packet loss may be observed.
To resolve this issue, a &#8220;wait and retry&#8221; mode is implemented in the testpmd and vhost sample code.
In the &#8220;wait and retry&#8221; mode if the virtqueue is found to be full, then testpmd waits for a period of time before retrying to enqueue packets.</p>
<p>The &#8220;wait and retry&#8221; algorithm is implemented in DPDK testpmd as a forwarding method call &#8220;mac_retry&#8221;.
The following sequence diagram describes the algorithm in detail.</p>
<div class="figure" id="id5">
<span id="figure-tx-dpdk-testpmd"></span><img alt="../_images/tx_dpdk_testpmd.png" src="../_images/tx_dpdk_testpmd.png" />
<p class="caption"><span class="caption-number">Fig. 31.5 </span><span class="caption-text">Packet Flow on TX in DPDK-testpmd</span></p>
</div>
</div>
<div class="section" id="running-testpmd">
<h3>31.8.2. Running Testpmd</h3>
<p>The testpmd application is automatically built when DPDK is installed.
Run the testpmd application as follows:</p>
<div class="highlight-console"><div class="highlight"><pre><span></span><span class="go">cd ${RTE_SDK}/x86_64-native-linuxapp-gcc/app</span>
<span class="go">./testpmd -c 0x3 -n 4 --socket-mem 512 \</span>
<span class="go">-- --burst=64 --i --disable-hw-vlan-filter</span>
</pre></div>
</div>
<p>The destination MAC address for packets transmitted on each port can be set at the command line:</p>
<div class="highlight-console"><div class="highlight"><pre><span></span><span class="go">./testpmd -c 0x3 -n 4 --socket-mem 512 \</span>
<span class="go">-- --burst=64 --i --disable-hw-vlan-filter \</span>
<span class="go">--eth-peer=0,aa:bb:cc:dd:ee:ff --eth-peer=1,ff:ee:dd:cc:bb:aa</span>
</pre></div>
</div>
<ul>
<li><p class="first">Packets received on port 1 will be forwarded on port 0 to MAC address</p>
<p>aa:bb:cc:dd:ee:ff</p>
</li>
<li><p class="first">Packets received on port 0 will be forwarded on port 1 to MAC address</p>
<p>ff:ee:dd:cc:bb:aa</p>
</li>
</ul>
<p>The testpmd application can then be configured to act as an L2 forwarding application:</p>
<div class="highlight-console"><div class="highlight"><pre><span></span><span class="go">testpmd&gt; set fwd mac_retry</span>
</pre></div>
</div>
<p>The testpmd can then be configured to start processing packets,
transmitting packets first so the DPDK vhost sample code on the host can learn the MAC address:</p>
<div class="highlight-console"><div class="highlight"><pre><span></span><span class="go">testpmd&gt; start tx_first</span>
</pre></div>
</div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Please note &#8220;set fwd mac_retry&#8221; is used in place of &#8220;set fwd mac_fwd&#8221; to ensure the retry feature is activated.</p>
</div>
</div>
</div>
<div class="section" id="passing-traffic-to-the-virtual-machine-device">
<h2>31.9. Passing Traffic to the Virtual Machine Device</h2>
<p>For a virtio-net device to receive traffic,
the traffic&#8217;s Layer 2 header must include both the virtio-net device&#8217;s MAC address and VLAN tag.
The DPDK sample code behaves in a similar manner to a learning switch in that
it learns the MAC address of the virtio-net devices from the first transmitted packet.
On learning the MAC address,
the DPDK vhost sample code prints a message with the MAC address and VLAN tag virtio-net device.
For example:</p>
<div class="highlight-console"><div class="highlight"><pre><span></span><span class="go">DATA: (0) MAC_ADDRESS cc:bb:bb:bb:bb:bb and VLAN_TAG 1000 registered</span>
</pre></div>
</div>
<p>The above message indicates that device 0 has been registered with MAC address cc:bb:bb:bb:bb:bb and VLAN tag 1000.
Any packets received on the NIC with these values is placed on the devices receive queue.
When a virtio-net device transmits packets, the VLAN tag is added to the packet by the DPDK vhost sample code.</p>
</div>
</div>


           </div>
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="netmap_compatibility.html" class="btn btn-neutral float-right" title="32. Netmap Compatibility Sample Application" accesskey="n">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="vmdq_dcb_forwarding.html" class="btn btn-neutral" title="30. VMDQ and DCB Forwarding Sample Application" accesskey="p"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  

    <script type="text/javascript">
        var DOCUMENTATION_OPTIONS = {
            URL_ROOT:'../',
            VERSION:'16.04.0',
            COLLAPSE_INDEX:false,
            FILE_SUFFIX:'.html',
            HAS_SOURCE:  true
        };
    </script>
      <script type="text/javascript" src="../_static/jquery.js"></script>
      <script type="text/javascript" src="../_static/underscore.js"></script>
      <script type="text/javascript" src="../_static/doctools.js"></script>

  

  
  
    <script type="text/javascript" src="../_static/js/theme.js"></script>
  

  
  
  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.StickyNav.enable();
      });
  </script>
   

</body>
</html>