

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>40. Performance Thread Sample Application &mdash; Data Plane Development Kit 16.04.0 documentation</title>
  

  
  

  

  
  
    

  

  
  
    <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  

  

  
    <link rel="top" title="Data Plane Development Kit 16.04.0 documentation" href="../index.html"/>
        <link rel="up" title="Sample Applications User Guide" href="index.html"/>
        <link rel="next" title="41. IPsec Security Gateway Sample Application" href="ipsec_secgw.html"/>
        <link rel="prev" title="39. PTP Client Sample Application" href="ptpclient.html"/> 

  
  <script src="../_static/js/modernizr.min.js"></script>

</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
          

          
            <a href="../index.html" class="icon icon-home"> Data Plane Development Kit
          

          
            
            <img src="../_static/DPDK_logo_vertical_rev_small.png" class="logo" />
          
          </a>

          
            
            
              <div class="version">
                16.04.0
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
                <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../linux_gsg/index.html">Getting Started Guide for Linux</a></li>
<li class="toctree-l1"><a class="reference internal" href="../freebsd_gsg/index.html">Getting Started Guide for FreeBSD</a></li>
<li class="toctree-l1"><a class="reference internal" href="../xen/index.html">Xen Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../prog_guide/index.html">Programmer&#8217;s Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../nics/index.html">Network Interface Controller Drivers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cryptodevs/index.html">Crypto Device Drivers</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="index.html">Sample Applications User Guide</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="intro.html">1. Introduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="cmd_line.html">2. Command Line Sample Application</a></li>
<li class="toctree-l2"><a class="reference internal" href="ethtool.html">3. Ethtool Sample Application</a></li>
<li class="toctree-l2"><a class="reference internal" href="exception_path.html">4. Exception Path Sample Application</a></li>
<li class="toctree-l2"><a class="reference internal" href="hello_world.html">5. Hello World Sample Application</a></li>
<li class="toctree-l2"><a class="reference internal" href="skeleton.html">6. Basic Forwarding Sample Application</a></li>
<li class="toctree-l2"><a class="reference internal" href="rxtx_callbacks.html">7. RX/TX Callbacks Sample Application</a></li>
<li class="toctree-l2"><a class="reference internal" href="ip_frag.html">8. IP Fragmentation Sample Application</a></li>
<li class="toctree-l2"><a class="reference internal" href="ipv4_multicast.html">9. IPv4 Multicast Sample Application</a></li>
<li class="toctree-l2"><a class="reference internal" href="ip_reassembly.html">10. IP Reassembly Sample Application</a></li>
<li class="toctree-l2"><a class="reference internal" href="kernel_nic_interface.html">11. Kernel NIC Interface Sample Application</a></li>
<li class="toctree-l2"><a class="reference internal" href="keep_alive.html">12. Keep Alive Sample Application</a></li>
<li class="toctree-l2"><a class="reference internal" href="l2_forward_crypto.html">13. L2 Forwarding with Crypto Sample Application</a></li>
<li class="toctree-l2"><a class="reference internal" href="l2_forward_job_stats.html">14. L2 Forwarding Sample Application (in Real and Virtualized Environments) with core load statistics.</a></li>
<li class="toctree-l2"><a class="reference internal" href="l2_forward_real_virtual.html">15. L2 Forwarding Sample Application (in Real and Virtualized Environments)</a></li>
<li class="toctree-l2"><a class="reference internal" href="l2_forward_cat.html">16. L2 Forwarding Sample Application with Cache Allocation Technology (CAT)</a></li>
<li class="toctree-l2"><a class="reference internal" href="l3_forward.html">17. L3 Forwarding Sample Application</a></li>
<li class="toctree-l2"><a class="reference internal" href="l3_forward_power_man.html">18. L3 Forwarding with Power Management Sample Application</a></li>
<li class="toctree-l2"><a class="reference internal" href="l3_forward_access_ctrl.html">19. L3 Forwarding with Access Control Sample Application</a></li>
<li class="toctree-l2"><a class="reference internal" href="l3_forward_virtual.html">20. L3 Forwarding in a Virtualization Environment Sample Application</a></li>
<li class="toctree-l2"><a class="reference internal" href="link_status_intr.html">21. Link Status Interrupt Sample Application</a></li>
<li class="toctree-l2"><a class="reference internal" href="load_balancer.html">22. Load Balancer Sample Application</a></li>
<li class="toctree-l2"><a class="reference internal" href="multi_process.html">23. Multi-process Sample Application</a></li>
<li class="toctree-l2"><a class="reference internal" href="qos_metering.html">24. QoS Metering Sample Application</a></li>
<li class="toctree-l2"><a class="reference internal" href="qos_scheduler.html">25. QoS Scheduler Sample Application</a></li>
<li class="toctree-l2"><a class="reference internal" href="intel_quickassist.html">26. IntelÂ® QuickAssist Technology Sample Application</a></li>
<li class="toctree-l2"><a class="reference internal" href="quota_watermark.html">27. Quota and Watermark Sample Application</a></li>
<li class="toctree-l2"><a class="reference internal" href="timer.html">28. Timer Sample Application</a></li>
<li class="toctree-l2"><a class="reference internal" href="packet_ordering.html">29. Packet Ordering Application</a></li>
<li class="toctree-l2"><a class="reference internal" href="vmdq_dcb_forwarding.html">30. VMDQ and DCB Forwarding Sample Application</a></li>
<li class="toctree-l2"><a class="reference internal" href="vhost.html">31. Vhost Sample Application</a></li>
<li class="toctree-l2"><a class="reference internal" href="netmap_compatibility.html">32. Netmap Compatibility Sample Application</a></li>
<li class="toctree-l2"><a class="reference internal" href="ip_pipeline.html">33. Internet Protocol (IP) Pipeline Application</a></li>
<li class="toctree-l2"><a class="reference internal" href="test_pipeline.html">34. Test Pipeline Application</a></li>
<li class="toctree-l2"><a class="reference internal" href="dist_app.html">35. Distributor Sample Application</a></li>
<li class="toctree-l2"><a class="reference internal" href="vm_power_management.html">36. VM Power Management Application</a></li>
<li class="toctree-l2"><a class="reference internal" href="tep_termination.html">37. TEP termination Sample Application</a></li>
<li class="toctree-l2"><a class="reference internal" href="proc_info.html">38. dpdk_proc_info Application</a></li>
<li class="toctree-l2"><a class="reference internal" href="ptpclient.html">39. PTP Client Sample Application</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">40. Performance Thread Sample Application</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#overview">40.1. Overview</a></li>
<li class="toctree-l3"><a class="reference internal" href="#compiling-the-application">40.2. Compiling the Application</a></li>
<li class="toctree-l3"><a class="reference internal" href="#running-the-application">40.3. Running the Application</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#running-with-l-threads">40.3.1. Running with L-threads</a></li>
<li class="toctree-l4"><a class="reference internal" href="#running-with-eal-threads">40.3.2. Running with EAL threads</a></li>
<li class="toctree-l4"><a class="reference internal" href="#examples">40.3.3. Examples</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#explanation">40.4. Explanation</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#mode-of-operation-with-eal-threads">40.4.1. Mode of operation with EAL threads</a></li>
<li class="toctree-l4"><a class="reference internal" href="#mode-of-operation-with-l-threads">40.4.2. Mode of operation with L-threads</a></li>
<li class="toctree-l4"><a class="reference internal" href="#cpu-load-statistics">40.4.3. CPU load statistics</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#the-l-thread-subsystem">40.5. The L-thread subsystem</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#comparison-between-l-threads-and-posix-pthreads">40.5.1. Comparison between L-threads and POSIX pthreads</a></li>
<li class="toctree-l4"><a class="reference internal" href="#constraints-and-performance-implications-when-using-l-threads">40.5.2. Constraints and performance implications when using L-threads</a></li>
<li class="toctree-l4"><a class="reference internal" href="#porting-legacy-code-to-run-on-l-threads">40.5.3. Porting legacy code to run on L-threads</a></li>
<li class="toctree-l4"><a class="reference internal" href="#pthread-shim">40.5.4. Pthread shim</a></li>
<li class="toctree-l4"><a class="reference internal" href="#l-thread-diagnostics">40.5.5. L-thread Diagnostics</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="ipsec_secgw.html">41. IPsec Security Gateway Sample Application</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../testpmd_app_ug/index.html">Testpmd Application User Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/index.html">FAQ</a></li>
<li class="toctree-l1"><a class="reference internal" href="../rel_notes/index.html">Release Notes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../contributing/index.html">Contributor&#8217;s Guidelines</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
        <a href="../index.html">Data Plane Development Kit</a>
      </nav>


      
      <div class="wy-nav-content">
        <div class="rst-content">
          

 



<div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="../index.html">Docs</a> &raquo;</li>
      
          <li><a href="index.html">Sample Applications User Guide</a> &raquo;</li>
      
    <li>40. Performance Thread Sample Application</li>
      <li class="wy-breadcrumbs-aside">
        
          
            <a href="../_sources/sample_app_ug/performance_thread.txt" rel="nofollow"> View page source</a>
          
        
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="performance-thread-sample-application">
<h1>40. Performance Thread Sample Application</h1>
<p>The performance thread sample application is a derivative of the standard L3
forwarding application that demonstrates different threading models.</p>
<div class="section" id="overview">
<h2>40.1. Overview</h2>
<p>For a general description of the L3 forwarding applications capabilities
please refer to the documentation of the standard application in
<a class="reference internal" href="l3_forward.html"><span class="doc">L3 Forwarding Sample Application</span></a>.</p>
<p>The performance thread sample application differs from the standard L3
forwarding example in that it divides the TX and RX processing between
different threads, and makes it possible to assign individual threads to
different cores.</p>
<p>Three threading models are considered:</p>
<ol class="arabic simple">
<li>When there is one EAL thread per physical core.</li>
<li>When there are multiple EAL threads per physical core.</li>
<li>When there are multiple lightweight threads per EAL thread.</li>
</ol>
<p>Since DPDK release 2.0 it is possible to launch applications using the
<code class="docutils literal"><span class="pre">--lcores</span></code> EAL parameter, specifying cpu-sets for a physical core. With the
performance thread sample application its is now also possible to assign
individual RX and TX functions to different cores.</p>
<p>As an alternative to dividing the L3 forwarding work between different EAL
threads the performance thread sample introduces the possibility to run the
application threads as lightweight threads (L-threads) within one or
more EAL threads.</p>
<p>In order to facilitate this threading model the example includes a primitive
cooperative scheduler (L-thread) subsystem. More details of the L-thread
subsystem can be found in <a class="reference internal" href="#lthread-subsystem"><span class="std std-ref">The L-thread subsystem</span></a>.</p>
<p><strong>Note:</strong> Whilst theoretically possible it is not anticipated that multiple
L-thread schedulers would be run on the same physical core, this mode of
operation should not be expected to yield useful performance and is considered
invalid.</p>
</div>
<div class="section" id="compiling-the-application">
<h2>40.2. Compiling the Application</h2>
<p>The application is located in the sample application folder in the
<code class="docutils literal"><span class="pre">performance-thread</span></code> folder.</p>
<ol class="arabic">
<li><p class="first">Go to the example applications folder</p>
<div class="highlight-console"><div class="highlight"><pre><span></span><span class="go">export RTE_SDK=/path/to/rte_sdk</span>
<span class="go">cd ${RTE_SDK}/examples/performance-thread/l3fwd-thread</span>
</pre></div>
</div>
</li>
<li><p class="first">Set the target (a default target is used if not specified). For example:</p>
<div class="highlight-console"><div class="highlight"><pre><span></span><span class="go">export RTE_TARGET=x86_64-native-linuxapp-gcc</span>
</pre></div>
</div>
<p>See the <em>DPDK Linux Getting Started Guide</em> for possible RTE_TARGET values.</p>
</li>
<li><p class="first">Build the application:</p>
<blockquote>
<div><p>make</p>
</div></blockquote>
</li>
</ol>
</div>
<div class="section" id="running-the-application">
<h2>40.3. Running the Application</h2>
<p>The application has a number of command line options:</p>
<div class="highlight-none"><div class="highlight"><pre><span></span>./build/l3fwd-thread [EAL options] --
    -p PORTMASK [-P]
    --rx(port,queue,lcore,thread)[,(port,queue,lcore,thread)]
    --tx(lcore,thread)[,(lcore,thread)]
    [--enable-jumbo] [--max-pkt-len PKTLEN]]  [--no-numa]
    [--hash-entry-num] [--ipv6] [--no-lthreads] [--stat-lcore lcore]
</pre></div>
</div>
<p>Where:</p>
<ul class="simple">
<li><code class="docutils literal"><span class="pre">-p</span> <span class="pre">PORTMASK</span></code>: Hexadecimal bitmask of ports to configure.</li>
<li><code class="docutils literal"><span class="pre">-P</span></code>: optional, sets all ports to promiscuous mode so that packets are
accepted regardless of the packet&#8217;s Ethernet MAC destination address.
Without this option, only packets with the Ethernet MAC destination address
set to the Ethernet address of the port are accepted.</li>
<li><code class="docutils literal"><span class="pre">--rx</span> <span class="pre">(port,queue,lcore,thread)[,(port,queue,lcore,thread)]</span></code>: the list of
NIC RX ports and queues handled by the RX lcores and threads. The parameters
are explained below.</li>
<li><code class="docutils literal"><span class="pre">--tx</span> <span class="pre">(lcore,thread)[,(lcore,thread)]</span></code>: the list of TX threads identifying
the lcore the thread runs on, and the id of RX thread with which it is
associated. The parameters are explained below.</li>
<li><code class="docutils literal"><span class="pre">--enable-jumbo</span></code>: optional, enables jumbo frames.</li>
<li><code class="docutils literal"><span class="pre">--max-pkt-len</span></code>: optional, maximum packet length in decimal (64-9600).</li>
<li><code class="docutils literal"><span class="pre">--no-numa</span></code>: optional, disables numa awareness.</li>
<li><code class="docutils literal"><span class="pre">--hash-entry-num</span></code>: optional, specifies the hash entry number in hex to be
setup.</li>
<li><code class="docutils literal"><span class="pre">--ipv6</span></code>: optional, set it if running ipv6 packets.</li>
<li><code class="docutils literal"><span class="pre">--no-lthreads</span></code>: optional, disables l-thread model and uses EAL threading
model. See below.</li>
<li><code class="docutils literal"><span class="pre">--stat-lcore</span></code>: optional, run CPU load stats collector on the specified
lcore.</li>
</ul>
<p>The parameters of the <code class="docutils literal"><span class="pre">--rx</span></code> and <code class="docutils literal"><span class="pre">--tx</span></code> options are:</p>
<ul>
<li><p class="first"><code class="docutils literal"><span class="pre">--rx</span></code> parameters</p>
<blockquote>
<div><table border="1" class="docutils" id="table-l3fwd-rx-parameters">
<colgroup>
<col width="13%" />
<col width="87%" />
</colgroup>
<tbody valign="top">
<tr class="row-odd"><td>port</td>
<td>RX port</td>
</tr>
<tr class="row-even"><td>queue</td>
<td>RX queue that will be read on the specified RX port</td>
</tr>
<tr class="row-odd"><td>lcore</td>
<td>Core to use for the thread</td>
</tr>
<tr class="row-even"><td>thread</td>
<td>Thread id (continuously from 0 to N)</td>
</tr>
</tbody>
</table>
</div></blockquote>
</li>
<li><p class="first"><code class="docutils literal"><span class="pre">--tx</span></code> parameters</p>
<blockquote>
<div><table border="1" class="docutils" id="table-l3fwd-tx-parameters">
<colgroup>
<col width="13%" />
<col width="87%" />
</colgroup>
<tbody valign="top">
<tr class="row-odd"><td>lcore</td>
<td>Core to use for L3 route match and transmit</td>
</tr>
<tr class="row-even"><td>thread</td>
<td>Id of RX thread to be associated with this TX thread</td>
</tr>
</tbody>
</table>
</div></blockquote>
</li>
</ul>
<p>The <code class="docutils literal"><span class="pre">l3fwd-thread</span></code> application allows you to start packet processing in two
threading models: L-Threads (default) and EAL Threads (when the
<code class="docutils literal"><span class="pre">--no-lthreads</span></code> parameter is used). For consistency all parameters are used
in the same way for both models.</p>
<div class="section" id="running-with-l-threads">
<h3>40.3.1. Running with L-threads</h3>
<p>When the L-thread model is used (default option), lcore and thread parameters
in <code class="docutils literal"><span class="pre">--rx/--tx</span></code> are used to affinitize threads to the selected scheduler.</p>
<p>For example, the following places every l-thread on different lcores:</p>
<div class="highlight-none"><div class="highlight"><pre><span></span>l3fwd-thread -c ff -n 2 -- -P -p 3 \
             --rx=&quot;(0,0,0,0)(1,0,1,1)&quot; \
             --tx=&quot;(2,0)(3,1)&quot;
</pre></div>
</div>
<p>The following places RX l-threads on lcore 0 and TX l-threads on lcore 1 and 2
and so on:</p>
<div class="highlight-none"><div class="highlight"><pre><span></span>l3fwd-thread -c ff -n 2 -- -P -p 3 \
             --rx=&quot;(0,0,0,0)(1,0,0,1)&quot; \
             --tx=&quot;(1,0)(2,1)&quot;
</pre></div>
</div>
</div>
<div class="section" id="running-with-eal-threads">
<h3>40.3.2. Running with EAL threads</h3>
<p>When the <code class="docutils literal"><span class="pre">--no-lthreads</span></code> parameter is used, the L-threading model is turned
off and EAL threads are used for all processing. EAL threads are enumerated in
the same way as L-threads, but the <code class="docutils literal"><span class="pre">--lcores</span></code> EAL parameter is used to
affinitize threads to the selected cpu-set (scheduler). Thus it is possible to
place every RX and TX thread on different lcores.</p>
<p>For example, the following places every EAL thread on different lcores:</p>
<div class="highlight-none"><div class="highlight"><pre><span></span>l3fwd-thread -c ff -n 2 -- -P -p 3 \
             --rx=&quot;(0,0,0,0)(1,0,1,1)&quot; \
             --tx=&quot;(2,0)(3,1)&quot; \
             --no-lthreads
</pre></div>
</div>
<p>To affinitize two or more EAL threads to one cpu-set, the EAL <code class="docutils literal"><span class="pre">--lcores</span></code>
parameter is used.</p>
<p>The following places RX EAL threads on lcore 0 and TX EAL threads on lcore 1
and 2 and so on:</p>
<div class="highlight-none"><div class="highlight"><pre><span></span>l3fwd-thread -c ff -n 2 --lcores=&quot;(0,1)@0,(2,3)@1&quot; -- -P -p 3 \
             --rx=&quot;(0,0,0,0)(1,0,1,1)&quot; \
             --tx=&quot;(2,0)(3,1)&quot; \
             --no-lthreads
</pre></div>
</div>
</div>
<div class="section" id="examples">
<h3>40.3.3. Examples</h3>
<p>For selected scenarios the command line configuration of the application for L-threads
and its corresponding EAL threads command line can be realized as follows:</p>
<ol class="loweralpha">
<li><p class="first">Start every thread on different scheduler (1:1):</p>
<div class="highlight-none"><div class="highlight"><pre><span></span>l3fwd-thread -c ff -n 2 -- -P -p 3 \
             --rx=&quot;(0,0,0,0)(1,0,1,1)&quot; \
             --tx=&quot;(2,0)(3,1)&quot;
</pre></div>
</div>
<p>EAL thread equivalent:</p>
<div class="highlight-none"><div class="highlight"><pre><span></span>l3fwd-thread -c ff -n 2 -- -P -p 3 \
             --rx=&quot;(0,0,0,0)(1,0,1,1)&quot; \
             --tx=&quot;(2,0)(3,1)&quot; \
             --no-lthreads
</pre></div>
</div>
</li>
<li><p class="first">Start all threads on one core (N:1).</p>
<p>Start 4 L-threads on lcore 0:</p>
<div class="highlight-none"><div class="highlight"><pre><span></span>l3fwd-thread -c ff -n 2 -- -P -p 3 \
             --rx=&quot;(0,0,0,0)(1,0,0,1)&quot; \
             --tx=&quot;(0,0)(0,1)&quot;
</pre></div>
</div>
<p>Start 4 EAL threads on cpu-set 0:</p>
<div class="highlight-none"><div class="highlight"><pre><span></span>l3fwd-thread -c ff -n 2 --lcores=&quot;(0-3)@0&quot; -- -P -p 3 \
             --rx=&quot;(0,0,0,0)(1,0,0,1)&quot; \
             --tx=&quot;(2,0)(3,1)&quot; \
             --no-lthreads
</pre></div>
</div>
</li>
<li><p class="first">Start threads on different cores (N:M).</p>
<p>Start 2 L-threads for RX on lcore 0, and 2 L-threads for TX on lcore 1:</p>
<div class="highlight-none"><div class="highlight"><pre><span></span>l3fwd-thread -c ff -n 2 -- -P -p 3 \
             --rx=&quot;(0,0,0,0)(1,0,0,1)&quot; \
             --tx=&quot;(1,0)(1,1)&quot;
</pre></div>
</div>
<p>Start 2 EAL threads for RX on cpu-set 0, and 2 EAL threads for TX on
cpu-set 1:</p>
<div class="highlight-none"><div class="highlight"><pre><span></span>l3fwd-thread -c ff -n 2 --lcores=&quot;(0-1)@0,(2-3)@1&quot; -- -P -p 3 \
             --rx=&quot;(0,0,0,0)(1,0,1,1)&quot; \
             --tx=&quot;(2,0)(3,1)&quot; \
             --no-lthreads
</pre></div>
</div>
</li>
</ol>
</div>
</div>
<div class="section" id="explanation">
<h2>40.4. Explanation</h2>
<p>To a great extent the sample application differs little from the standard L3
forwarding application, and readers are advised to familiarize themselves with
the material covered in the <a class="reference internal" href="l3_forward.html"><span class="doc">L3 Forwarding Sample Application</span></a> documentation before proceeding.</p>
<p>The following explanation is focused on the way threading is handled in the
performance thread example.</p>
<div class="section" id="mode-of-operation-with-eal-threads">
<h3>40.4.1. Mode of operation with EAL threads</h3>
<p>The performance thread sample application has split the RX and TX functionality
into two different threads, and the RX and TX threads are
interconnected via software rings. With respect to these rings the RX threads
are producers and the TX threads are consumers.</p>
<p>On initialization the TX and RX threads are started according to the command
line parameters.</p>
<p>The RX threads poll the network interface queues and post received packets to a
TX thread via a corresponding software ring.</p>
<p>The TX threads poll software rings, perform the L3 forwarding hash/LPM match,
and assemble packet bursts before performing burst transmit on the network
interface.</p>
<p>As with the standard L3 forward application, burst draining of residual packets
is performed periodically with the period calculated from elapsed time using
the timestamps counter.</p>
<p>The diagram below illustrates a case with two RX threads and three TX threads.</p>
<div class="figure" id="figure-performance-thread-1">
<img src="../_images/performance_thread_1.svg" /></div>
</div>
<div class="section" id="mode-of-operation-with-l-threads">
<h3>40.4.2. Mode of operation with L-threads</h3>
<p>Like the EAL thread configuration the application has split the RX and TX
functionality into different threads, and the pairs of RX and TX threads are
interconnected via software rings.</p>
<p>On initialization an L-thread scheduler is started on every EAL thread. On all
but the master EAL thread only a a dummy L-thread is initially started.
The L-thread started on the master EAL thread then spawns other L-threads on
different L-thread schedulers according the the command line parameters.</p>
<p>The RX threads poll the network interface queues and post received packets
to a TX thread via the corresponding software ring.</p>
<p>The ring interface is augmented by means of an L-thread condition variable that
enables the TX thread to be suspended when the TX ring is empty. The RX thread
signals the condition whenever it posts to the TX ring, causing the TX thread
to be resumed.</p>
<p>Additionally the TX L-thread spawns a worker L-thread to take care of
polling the software rings, whilst it handles burst draining of the transmit
buffer.</p>
<p>The worker threads poll the software rings, perform L3 route lookup and
assemble packet bursts. If the TX ring is empty the worker thread suspends
itself by waiting on the condition variable associated with the ring.</p>
<p>Burst draining of residual packets, less than the burst size, is performed by
the TX thread which sleeps (using an L-thread sleep function) and resumes
periodically to flush the TX buffer.</p>
<p>This design means that L-threads that have no work, can yield the CPU to other
L-threads and avoid having to constantly poll the software rings.</p>
<p>The diagram below illustrates a case with two RX threads and three TX functions
(each comprising a thread that processes forwarding and a thread that
periodically drains the output buffer of residual packets).</p>
<div class="figure" id="figure-performance-thread-2">
<img src="../_images/performance_thread_2.svg" /></div>
</div>
<div class="section" id="cpu-load-statistics">
<h3>40.4.3. CPU load statistics</h3>
<p>It is possible to display statistics showing estimated CPU load on each core.
The statistics indicate the percentage of CPU time spent: processing
received packets (forwarding), polling queues/rings (waiting for work),
and doing any other processing (context switch and other overhead).</p>
<p>When enabled statistics are gathered by having the application threads set and
clear flags when they enter and exit pertinent code sections. The flags are
then sampled in real time by a statistics collector thread running on another
core. This thread displays the data in real time on the console.</p>
<p>This feature is enabled by designating a statistics collector core, using the
<code class="docutils literal"><span class="pre">--stat-lcore</span></code> parameter.</p>
</div>
</div>
<div class="section" id="the-l-thread-subsystem">
<span id="lthread-subsystem"></span><h2>40.5. The L-thread subsystem</h2>
<p>The L-thread subsystem resides in the examples/performance-thread/common
directory and is built and linked automatically when building the
<code class="docutils literal"><span class="pre">l3fwd-thread</span></code> example.</p>
<p>The subsystem provides a simple cooperative scheduler to enable arbitrary
functions to run as cooperative threads within a single EAL thread.
The subsystem provides a pthread like API that is intended to assist in
reuse of legacy code written for POSIX pthreads.</p>
<p>The following sections provide some detail on the features, constraints,
performance and porting considerations when using L-threads.</p>
<div class="section" id="comparison-between-l-threads-and-posix-pthreads">
<span id="comparison-between-lthreads-and-pthreads"></span><h3>40.5.1. Comparison between L-threads and POSIX pthreads</h3>
<p>The fundamental difference between the L-thread and pthread models is the
way in which threads are scheduled. The simplest way to think about this is to
consider the case of a processor with a single CPU. To run multiple threads
on a single CPU, the scheduler must frequently switch between the threads,
in order that each thread is able to make timely progress.
This is the basis of any multitasking operating system.</p>
<p>This section explores the differences between the pthread model and the
L-thread model as implemented in the provided L-thread subsystem. If needed a
theoretical discussion of preemptive vs cooperative multi-threading can be
found in any good text on operating system design.</p>
<div class="section" id="scheduling-and-context-switching">
<h4>40.5.1.1. Scheduling and context switching</h4>
<p>The POSIX pthread library provides an application programming interface to
create and synchronize threads. Scheduling policy is determined by the host OS,
and may be configurable. The OS may use sophisticated rules to determine which
thread should be run next, threads may suspend themselves or make other threads
ready, and the scheduler may employ a time slice giving each thread a maximum
time quantum after which it will be preempted in favor of another thread that
is ready to run. To complicate matters further threads may be assigned
different scheduling priorities.</p>
<p>By contrast the L-thread subsystem is considerably simpler. Logically the
L-thread scheduler performs the same multiplexing function for L-threads
within a single pthread as the OS scheduler does for pthreads within an
application process. The L-thread scheduler is simply the main loop of a
pthread, and in so far as the host OS is concerned it is a regular pthread
just like any other. The host OS is oblivious about the existence of and
not at all involved in the scheduling of L-threads.</p>
<p>The other and most significant difference between the two models is that
L-threads are scheduled cooperatively. L-threads cannot not preempt each
other, nor can the L-thread scheduler preempt a running L-thread (i.e.
there is no time slicing). The consequence is that programs implemented with
L-threads must possess frequent rescheduling points, meaning that they must
explicitly and of their own volition return to the scheduler at frequent
intervals, in order to allow other L-threads an opportunity to proceed.</p>
<p>In both models switching between threads requires that the current CPU
context is saved and a new context (belonging to the next thread ready to run)
is restored. With pthreads this context switching is handled transparently
and the set of CPU registers that must be preserved between context switches
is as per an interrupt handler.</p>
<p>An L-thread context switch is achieved by the thread itself making a function
call to the L-thread scheduler. Thus it is only necessary to preserve the
callee registers. The caller is responsible to save and restore any other
registers it is using before a function call, and restore them on return,
and this is handled by the compiler. For <code class="docutils literal"><span class="pre">X86_64</span></code> on both Linux and BSD the
System V calling convention is used, this defines registers RSP, RBP, and
R12-R15 as callee-save registers (for more detailed discussion a good reference
is <a class="reference external" href="https://en.wikipedia.org/wiki/X86_calling_conventions">X86 Calling Conventions</a>).</p>
<p>Taking advantage of this, and due to the absence of preemption, an L-thread
context switch is achieved with less than 20 load/store instructions.</p>
<p>The scheduling policy for L-threads is fixed, there is no prioritization of
L-threads, all L-threads are equal and scheduling is based on a FIFO
ready queue.</p>
<p>An L-thread is a struct containing the CPU context of the thread
(saved on context switch) and other useful items. The ready queue contains
pointers to threads that are ready to run. The L-thread scheduler is a simple
loop that polls the ready queue, reads from it the next thread ready to run,
which it resumes by saving the current context (the current position in the
scheduler loop) and restoring the context of the next thread from its thread
struct. Thus an L-thread is always resumed at the last place it yielded.</p>
<p>A well behaved L-thread will call the context switch regularly (at least once
in its main loop) thus returning to the scheduler&#8217;s own main loop. Yielding
inserts the current thread at the back of the ready queue, and the process of
servicing the ready queue is repeated, thus the system runs by flipping back
and forth the between L-threads and scheduler loop.</p>
<p>In the case of pthreads, the preemptive scheduling, time slicing, and support
for thread prioritization means that progress is normally possible for any
thread that is ready to run. This comes at the price of a relatively heavier
context switch and scheduling overhead.</p>
<p>With L-threads the progress of any particular thread is determined by the
frequency of rescheduling opportunities in the other L-threads. This means that
an errant L-thread monopolizing the CPU might cause scheduling of other threads
to be stalled. Due to the lower cost of context switching, however, voluntary
rescheduling to ensure progress of other threads, if managed sensibly, is not
a prohibitive overhead, and overall performance can exceed that of an
application using pthreads.</p>
</div>
<div class="section" id="mutual-exclusion">
<h4>40.5.1.2. Mutual exclusion</h4>
<p>With pthreads preemption means that threads that share data must observe
some form of mutual exclusion protocol.</p>
<p>The fact that L-threads cannot preempt each other means that in many cases
mutual exclusion devices can be completely avoided.</p>
<p>Locking to protect shared data can be a significant bottleneck in
multi-threaded applications so a carefully designed cooperatively scheduled
program can enjoy significant performance advantages.</p>
<p>So far we have considered only the simplistic case of a single core CPU,
when multiple CPUs are considered things are somewhat more complex.</p>
<p>First of all it is inevitable that there must be multiple L-thread schedulers,
one running on each EAL thread. So long as these schedulers remain isolated
from each other the above assertions about the potential advantages of
cooperative scheduling hold true.</p>
<p>A configuration with isolated cooperative schedulers is less flexible than the
pthread model where threads can be affinitized to run on any CPU. With isolated
schedulers scaling of applications to utilize fewer or more CPUs according to
system demand is very difficult to achieve.</p>
<p>The L-thread subsystem makes it possible for L-threads to migrate between
schedulers running on different CPUs. Needless to say if the migration means
that threads that share data end up running on different CPUs then this will
introduce the need for some kind of mutual exclusion system.</p>
<p>Of course <code class="docutils literal"><span class="pre">rte_ring</span></code> software rings can always be used to interconnect
threads running on different cores, however to protect other kinds of shared
data structures, lock free constructs or else explicit locking will be
required. This is a consideration for the application design.</p>
<p>In support of this extended functionality, the L-thread subsystem implements
thread safe mutexes and condition variables.</p>
<p>The cost of affinitizing and of condition variable signaling is significantly
lower than the equivalent pthread operations, and so applications using these
features will see a performance benefit.</p>
</div>
<div class="section" id="thread-local-storage">
<h4>40.5.1.3. Thread local storage</h4>
<p>As with applications written for pthreads an application written for L-threads
can take advantage of thread local storage, in this case local to an L-thread.
An application may save and retrieve a single pointer to application data in
the L-thread struct.</p>
<p>For legacy and backward compatibility reasons two alternative methods are also
offered, the first is modelled directly on the pthread get/set specific APIs,
the second approach is modelled on the <code class="docutils literal"><span class="pre">RTE_PER_LCORE</span></code> macros, whereby
<code class="docutils literal"><span class="pre">PER_LTHREAD</span></code> macros are introduced, in both cases the storage is local to
the L-thread.</p>
</div>
</div>
<div class="section" id="constraints-and-performance-implications-when-using-l-threads">
<span id="constraints-and-performance-implications"></span><h3>40.5.2. Constraints and performance implications when using L-threads</h3>
<div class="section" id="api-compatibility">
<span id="id1"></span><h4>40.5.2.1. API compatibility</h4>
<p>The L-thread subsystem provides a set of functions that are logically equivalent
to the corresponding functions offered by the POSIX pthread library, however not
all pthread functions have a corresponding L-thread equivalent, and not all
features available to pthreads are implemented for L-threads.</p>
<p>The pthread library offers considerable flexibility via programmable attributes
that can be associated with threads, mutexes, and condition variables.</p>
<p>By contrast the L-thread subsystem has fixed functionality, the scheduler policy
cannot be varied, and L-threads cannot be prioritized. There are no variable
attributes associated with any L-thread objects. L-threads, mutexes and
conditional variables, all have fixed functionality. (Note: reserved parameters
are included in the APIs to facilitate possible future support for attributes).</p>
<p>The table below lists the pthread and equivalent L-thread APIs with notes on
differences and/or constraints. Where there is no L-thread entry in the table,
then the L-thread subsystem provides no equivalent function.</p>
<table border="1" class="docutils" id="id10">
<span id="table-lthread-pthread"></span><caption><span class="caption-number">Table 40.3 </span><span class="caption-text">Pthread and equivalent L-thread APIs.</caption>
<colgroup>
<col width="39%" />
<col width="34%" />
<col width="27%" />
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head"><strong>Pthread function</strong></th>
<th class="head"><strong>L-thread function</strong></th>
<th class="head"><strong>Notes</strong></th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td>pthread_barrier_destroy</td>
<td>&nbsp;</td>
<td>&nbsp;</td>
</tr>
<tr class="row-odd"><td>pthread_barrier_init</td>
<td>&nbsp;</td>
<td>&nbsp;</td>
</tr>
<tr class="row-even"><td>pthread_barrier_wait</td>
<td>&nbsp;</td>
<td>&nbsp;</td>
</tr>
<tr class="row-odd"><td>pthread_cond_broadcast</td>
<td>lthread_cond_broadcast</td>
<td>See note 1</td>
</tr>
<tr class="row-even"><td>pthread_cond_destroy</td>
<td>lthread_cond_destroy</td>
<td>&nbsp;</td>
</tr>
<tr class="row-odd"><td>pthread_cond_init</td>
<td>lthread_cond_init</td>
<td>&nbsp;</td>
</tr>
<tr class="row-even"><td>pthread_cond_signal</td>
<td>lthread_cond_signal</td>
<td>See note 1</td>
</tr>
<tr class="row-odd"><td>pthread_cond_timedwait</td>
<td>&nbsp;</td>
<td>&nbsp;</td>
</tr>
<tr class="row-even"><td>pthread_cond_wait</td>
<td>lthread_cond_wait</td>
<td>See note 5</td>
</tr>
<tr class="row-odd"><td>pthread_create</td>
<td>lthread_create</td>
<td>See notes 2, 3</td>
</tr>
<tr class="row-even"><td>pthread_detach</td>
<td>lthread_detach</td>
<td>See note 4</td>
</tr>
<tr class="row-odd"><td>pthread_equal</td>
<td>&nbsp;</td>
<td>&nbsp;</td>
</tr>
<tr class="row-even"><td>pthread_exit</td>
<td>lthread_exit</td>
<td>&nbsp;</td>
</tr>
<tr class="row-odd"><td>pthread_getspecific</td>
<td>lthread_getspecific</td>
<td>&nbsp;</td>
</tr>
<tr class="row-even"><td>pthread_getcpuclockid</td>
<td>&nbsp;</td>
<td>&nbsp;</td>
</tr>
<tr class="row-odd"><td>pthread_join</td>
<td>lthread_join</td>
<td>&nbsp;</td>
</tr>
<tr class="row-even"><td>pthread_key_create</td>
<td>lthread_key_create</td>
<td>&nbsp;</td>
</tr>
<tr class="row-odd"><td>pthread_key_delete</td>
<td>lthread_key_delete</td>
<td>&nbsp;</td>
</tr>
<tr class="row-even"><td>pthread_mutex_destroy</td>
<td>lthread_mutex_destroy</td>
<td>&nbsp;</td>
</tr>
<tr class="row-odd"><td>pthread_mutex_init</td>
<td>lthread_mutex_init</td>
<td>&nbsp;</td>
</tr>
<tr class="row-even"><td>pthread_mutex_lock</td>
<td>lthread_mutex_lock</td>
<td>See note 6</td>
</tr>
<tr class="row-odd"><td>pthread_mutex_trylock</td>
<td>lthread_mutex_trylock</td>
<td>See note 6</td>
</tr>
<tr class="row-even"><td>pthread_mutex_timedlock</td>
<td>&nbsp;</td>
<td>&nbsp;</td>
</tr>
<tr class="row-odd"><td>pthread_mutex_unlock</td>
<td>lthread_mutex_unlock</td>
<td>&nbsp;</td>
</tr>
<tr class="row-even"><td>pthread_once</td>
<td>&nbsp;</td>
<td>&nbsp;</td>
</tr>
<tr class="row-odd"><td>pthread_rwlock_destroy</td>
<td>&nbsp;</td>
<td>&nbsp;</td>
</tr>
<tr class="row-even"><td>pthread_rwlock_init</td>
<td>&nbsp;</td>
<td>&nbsp;</td>
</tr>
<tr class="row-odd"><td>pthread_rwlock_rdlock</td>
<td>&nbsp;</td>
<td>&nbsp;</td>
</tr>
<tr class="row-even"><td>pthread_rwlock_timedrdlock</td>
<td>&nbsp;</td>
<td>&nbsp;</td>
</tr>
<tr class="row-odd"><td>pthread_rwlock_timedwrlock</td>
<td>&nbsp;</td>
<td>&nbsp;</td>
</tr>
<tr class="row-even"><td>pthread_rwlock_tryrdlock</td>
<td>&nbsp;</td>
<td>&nbsp;</td>
</tr>
<tr class="row-odd"><td>pthread_rwlock_trywrlock</td>
<td>&nbsp;</td>
<td>&nbsp;</td>
</tr>
<tr class="row-even"><td>pthread_rwlock_unlock</td>
<td>&nbsp;</td>
<td>&nbsp;</td>
</tr>
<tr class="row-odd"><td>pthread_rwlock_wrlock</td>
<td>&nbsp;</td>
<td>&nbsp;</td>
</tr>
<tr class="row-even"><td>pthread_self</td>
<td>lthread_current</td>
<td>&nbsp;</td>
</tr>
<tr class="row-odd"><td>pthread_setspecific</td>
<td>lthread_setspecific</td>
<td>&nbsp;</td>
</tr>
<tr class="row-even"><td>pthread_spin_init</td>
<td>&nbsp;</td>
<td>See note 10</td>
</tr>
<tr class="row-odd"><td>pthread_spin_destroy</td>
<td>&nbsp;</td>
<td>See note 10</td>
</tr>
<tr class="row-even"><td>pthread_spin_lock</td>
<td>&nbsp;</td>
<td>See note 10</td>
</tr>
<tr class="row-odd"><td>pthread_spin_trylock</td>
<td>&nbsp;</td>
<td>See note 10</td>
</tr>
<tr class="row-even"><td>pthread_spin_unlock</td>
<td>&nbsp;</td>
<td>See note 10</td>
</tr>
<tr class="row-odd"><td>pthread_cancel</td>
<td>lthread_cancel</td>
<td>&nbsp;</td>
</tr>
<tr class="row-even"><td>pthread_setcancelstate</td>
<td>&nbsp;</td>
<td>&nbsp;</td>
</tr>
<tr class="row-odd"><td>pthread_setcanceltype</td>
<td>&nbsp;</td>
<td>&nbsp;</td>
</tr>
<tr class="row-even"><td>pthread_testcancel</td>
<td>&nbsp;</td>
<td>&nbsp;</td>
</tr>
<tr class="row-odd"><td>pthread_getschedparam</td>
<td>&nbsp;</td>
<td>&nbsp;</td>
</tr>
<tr class="row-even"><td>pthread_setschedparam</td>
<td>&nbsp;</td>
<td>&nbsp;</td>
</tr>
<tr class="row-odd"><td>pthread_yield</td>
<td>lthread_yield</td>
<td>See note 7</td>
</tr>
<tr class="row-even"><td>pthread_setaffinity_np</td>
<td>lthread_set_affinity</td>
<td>See notes 2, 3, 8</td>
</tr>
<tr class="row-odd"><td>&nbsp;</td>
<td>lthread_sleep</td>
<td>See note 9</td>
</tr>
<tr class="row-even"><td>&nbsp;</td>
<td>lthread_sleep_clks</td>
<td>See note 9</td>
</tr>
</tbody>
</table>
<p><strong>Note 1</strong>:</p>
<p>Neither lthread signal nor broadcast may be called concurrently by L-threads
running on different schedulers, although multiple L-threads running in the
same scheduler may freely perform signal or broadcast operations. L-threads
running on the same or different schedulers may always safely wait on a
condition variable.</p>
<p><strong>Note 2</strong>:</p>
<p>Pthread attributes may be used to affinitize a pthread with a cpu-set. The
L-thread subsystem does not support a cpu-set. An L-thread may be affinitized
only with a single CPU at any time.</p>
<p><strong>Note 3</strong>:</p>
<p>If an L-thread is intended to run on a different NUMA node than the node that
creates the thread then, when calling <code class="docutils literal"><span class="pre">lthread_create()</span></code> it is advantageous
to specify the destination core as a parameter of <code class="docutils literal"><span class="pre">lthread_create()</span></code>. See
<a class="reference internal" href="#memory-allocation-and-numa-awareness"><span class="std std-ref">Memory allocation and NUMA awareness</span></a> for details.</p>
<p><strong>Note 4</strong>:</p>
<p>An L-thread can only detach itself, and cannot detach other L-threads.</p>
<p><strong>Note 5</strong>:</p>
<p>A wait operation on a pthread condition variable is always associated with and
protected by a mutex which must be owned by the thread at the time it invokes
<code class="docutils literal"><span class="pre">pthread_wait()</span></code>. By contrast L-thread condition variables are thread safe
(for waiters) and do not use an associated mutex. Multiple L-threads (including
L-threads running on other schedulers) can safely wait on a L-thread condition
variable. As a consequence the performance of an L-thread condition variables
is typically an order of magnitude faster than its pthread counterpart.</p>
<p><strong>Note 6</strong>:</p>
<p>Recursive locking is not supported with L-threads, attempts to take a lock
recursively will be detected and rejected.</p>
<p><strong>Note 7</strong>:</p>
<p><code class="docutils literal"><span class="pre">lthread_yield()</span></code> will save the current context, insert the current thread
to the back of the ready queue, and resume the next ready thread. Yielding
increases ready queue backlog, see <a class="reference internal" href="#ready-queue-backlog"><span class="std std-ref">Ready queue backlog</span></a> for more details
about the implications of this.</p>
<p>N.B. The context switch time as measured from immediately before the call to
<code class="docutils literal"><span class="pre">lthread_yield()</span></code> to the point at which the next ready thread is resumed,
can be an order of magnitude faster that the same measurement for
pthread_yield.</p>
<p><strong>Note 8</strong>:</p>
<p><code class="docutils literal"><span class="pre">lthread_set_affinity()</span></code> is similar to a yield apart from the fact that the
yielding thread is inserted into a peer ready queue of another scheduler.
The peer ready queue is actually a separate thread safe queue, which means that
threads appearing in the peer ready queue can jump any backlog in the local
ready queue on the destination scheduler.</p>
<p>The context switch time as measured from the time just before the call to
<code class="docutils literal"><span class="pre">lthread_set_affinity()</span></code> to just after the same thread is resumed on the new
scheduler can be orders of magnitude faster than the same measurement for
<code class="docutils literal"><span class="pre">pthread_setaffinity_np()</span></code>.</p>
<p><strong>Note 9</strong>:</p>
<p>Although there is no <code class="docutils literal"><span class="pre">pthread_sleep()</span></code> function, <code class="docutils literal"><span class="pre">lthread_sleep()</span></code> and
<code class="docutils literal"><span class="pre">lthread_sleep_clks()</span></code> can be used wherever <code class="docutils literal"><span class="pre">sleep()</span></code>, <code class="docutils literal"><span class="pre">usleep()</span></code> or
<code class="docutils literal"><span class="pre">nanosleep()</span></code> might ordinarily be used. The L-thread sleep functions suspend
the current thread, start an <code class="docutils literal"><span class="pre">rte_timer</span></code> and resume the thread when the
timer matures. The <code class="docutils literal"><span class="pre">rte_timer_manage()</span></code> entry point is called on every pass
of the scheduler loop. This means that the worst case jitter on timer expiry
is determined by the longest period between context switches of any running
L-threads.</p>
<p>In a synthetic test with many threads sleeping and resuming then the measured
jitter is typically orders of magnitude lower than the same measurement made
for <code class="docutils literal"><span class="pre">nanosleep()</span></code>.</p>
<p><strong>Note 10</strong>:</p>
<p>Spin locks are not provided because they are problematical in a cooperative
environment, see <a class="reference internal" href="#porting-locks-and-spinlocks"><span class="std std-ref">Locks and spinlocks</span></a> for a more detailed
discussion on how to avoid spin locks.</p>
</div>
<div class="section" id="thread-local-storage-performance">
<span id="id2"></span><h4>40.5.2.2. Thread local storage</h4>
<p>Of the three L-thread local storage options the simplest and most efficient is
storing a single application data pointer in the L-thread struct.</p>
<p>The <code class="docutils literal"><span class="pre">PER_LTHREAD</span></code> macros involve a run time computation to obtain the address
of the variable being saved/retrieved and also require that the accesses are
de-referenced  via a pointer. This means that code that has used
<code class="docutils literal"><span class="pre">RTE_PER_LCORE</span></code> macros being ported to L-threads might need some slight
adjustment (see <a class="reference internal" href="#porting-thread-local-storage"><span class="std std-ref">Thread local storage</span></a> for hints about porting
code that makes use of thread local storage).</p>
<p>The get/set specific APIs are consistent with their pthread counterparts both
in use and in performance.</p>
</div>
<div class="section" id="memory-allocation-and-numa-awareness">
<span id="id3"></span><h4>40.5.2.3. Memory allocation and NUMA awareness</h4>
<p>All memory allocation is from DPDK huge pages, and is NUMA aware. Each
scheduler maintains its own caches of objects: lthreads, their stacks, TLS,
mutexes and condition variables. These caches are implemented as unbounded lock
free MPSC queues. When objects are created they are always allocated from the
caches on the local core (current EAL thread).</p>
<p>If an L-thread has been affinitized to a different scheduler, then it can
always safely free resources to the caches from which they originated (because
the caches are MPSC queues).</p>
<p>If the L-thread has been affinitized to a different NUMA node then the memory
resources associated with it may incur longer access latency.</p>
<p>The commonly used pattern of setting affinity on entry to a thread after it has
started, means that memory allocation for both the stack and TLS will have been
made from caches on the NUMA node on which the threads creator is running.
This has the side effect that access latency will be sub-optimal after
affinitizing.</p>
<p>This side effect can be mitigated to some extent (although not completely) by
specifying the destination CPU as a parameter of <code class="docutils literal"><span class="pre">lthread_create()</span></code> this
causes the L-thread&#8217;s stack and TLS to be allocated when it is first scheduled
on the destination scheduler, if the destination is a on another NUMA node it
results in a more optimal memory allocation.</p>
<p>Note that the lthread struct itself remains allocated from memory on the
creating node, this is unavoidable because an L-thread is known everywhere by
the address of this struct.</p>
</div>
<div class="section" id="object-cache-sizing">
<span id="id4"></span><h4>40.5.2.4. Object cache sizing</h4>
<p>The per lcore object caches pre-allocate objects in bulk whenever a request to
allocate an object finds a cache empty. By default 100 objects are
pre-allocated, this is defined by <code class="docutils literal"><span class="pre">LTHREAD_PREALLOC</span></code> in the public API
header file lthread_api.h. This means that the caches constantly grow to meet
system demand.</p>
<p>In the present implementation there is no mechanism to reduce the cache sizes
if system demand reduces. Thus the caches will remain at their maximum extent
indefinitely.</p>
<p>A consequence of the bulk pre-allocation of objects is that every 100 (default
value) additional new object create operations results in a call to
<code class="docutils literal"><span class="pre">rte_malloc()</span></code>. For creation of objects such as L-threads, which trigger the
allocation of even more objects (i.e. their stacks and TLS) then this can
cause outliers in scheduling performance.</p>
<p>If this is a problem the simplest mitigation strategy is to dimension the
system, by setting the bulk object pre-allocation size to some large number
that you do not expect to be exceeded. This means the caches will be populated
once only, the very first time a thread is created.</p>
</div>
<div class="section" id="ready-queue-backlog">
<span id="id5"></span><h4>40.5.2.5. Ready queue backlog</h4>
<p>One of the more subtle performance considerations is managing the ready queue
backlog. The fewer threads that are waiting in the ready queue then the faster
any particular thread will get serviced.</p>
<p>In a naive L-thread application with N L-threads simply looping and yielding,
this backlog will always be equal to the number of L-threads, thus the cost of
a yield to a particular L-thread will be N times the context switch time.</p>
<p>This side effect can be mitigated by arranging for threads to be suspended and
wait to be resumed, rather than polling for work by constantly yielding.
Blocking on a mutex or condition variable or even more obviously having a
thread sleep if it has a low frequency workload are all mechanisms by which a
thread can be excluded from the ready queue until it really does need to be
run. This can have a significant positive impact on performance.</p>
</div>
<div class="section" id="initialization-shutdown-and-dependencies">
<span id="initialization-and-shutdown-dependencies"></span><h4>40.5.2.6. Initialization, shutdown and dependencies</h4>
<p>The L-thread subsystem depends on DPDK for huge page allocation and depends on
the <code class="docutils literal"><span class="pre">rte_timer</span> <span class="pre">subsystem</span></code>. The DPDK EAL initialization and
<code class="docutils literal"><span class="pre">rte_timer_subsystem_init()</span></code> <strong>MUST</strong> be completed before the L-thread sub
system can be used.</p>
<p>Thereafter initialization of the L-thread subsystem is largely transparent to
the application. Constructor functions ensure that global variables are properly
initialized. Other than global variables each scheduler is initialized
independently the first time that an L-thread is created by a particular EAL
thread.</p>
<p>If the schedulers are to be run as isolated and independent schedulers, with
no intention that L-threads running on different schedulers will migrate between
schedulers or synchronize with L-threads running on other schedulers, then
initialization consists simply of creating an L-thread, and then running the
L-thread scheduler.</p>
<p>If there will be interaction between L-threads running on different schedulers,
then it is important that the starting of schedulers on different EAL threads
is synchronized.</p>
<p>To achieve this an additional initialization step is necessary, this is simply
to set the number of schedulers by calling the API function
<code class="docutils literal"><span class="pre">lthread_num_schedulers_set(n)</span></code>, where <code class="docutils literal"><span class="pre">n</span></code> is the number of EAL threads
that will run L-thread schedulers. Setting the number of schedulers to a
number greater than 0 will cause all schedulers to wait until the others have
started before beginning to schedule L-threads.</p>
<p>The L-thread scheduler is started by calling the function <code class="docutils literal"><span class="pre">lthread_run()</span></code>
and should be called from the EAL thread and thus become the main loop of the
EAL thread.</p>
<p>The function <code class="docutils literal"><span class="pre">lthread_run()</span></code>, will not return until all threads running on
the scheduler have exited, and the scheduler has been explicitly stopped by
calling <code class="docutils literal"><span class="pre">lthread_scheduler_shutdown(lcore)</span></code> or
<code class="docutils literal"><span class="pre">lthread_scheduler_shutdown_all()</span></code>.</p>
<p>All these function do is tell the scheduler that it can exit when there are no
longer any running L-threads, neither function forces any running L-thread to
terminate. Any desired application shutdown behavior must be designed and
built into the application to ensure that L-threads complete in a timely
manner.</p>
<p><strong>Important Note:</strong> It is assumed when the scheduler exits that the application
is terminating for good, the scheduler does not free resources before exiting
and running the scheduler a subsequent time will result in undefined behavior.</p>
</div>
</div>
<div class="section" id="porting-legacy-code-to-run-on-l-threads">
<span id="porting-legacy-code-to-run-on-lthreads"></span><h3>40.5.3. Porting legacy code to run on L-threads</h3>
<p>Legacy code originally written for a pthread environment may be ported to
L-threads if the considerations about differences in scheduling policy, and
constraints discussed in the previous sections can be accommodated.</p>
<p>This section looks in more detail at some of the issues that may have to be
resolved when porting code.</p>
<div class="section" id="pthread-api-compatibility">
<span id="id6"></span><h4>40.5.3.1. pthread API compatibility</h4>
<p>The first step is to establish exactly which pthread APIs the legacy
application uses, and to understand the requirements of those APIs. If there
are corresponding L-lthread APIs, and where the default pthread functionality
is used by the application then, notwithstanding the other issues discussed
here, it should be feasible to run the application with L-threads. If the
legacy code modifies the default behavior using attributes then if may be
necessary to make some adjustments to eliminate those requirements.</p>
</div>
<div class="section" id="blocking-system-api-calls">
<span id="blocking-system-calls"></span><h4>40.5.3.2. Blocking system API calls</h4>
<p>It is important to understand what other system services the application may be
using, bearing in mind that in a cooperatively scheduled environment a thread
cannot block without stalling the scheduler and with it all other cooperative
threads. Any kind of blocking system call, for example file or socket IO, is a
potential problem, a good tool to analyze the application for this purpose is
the <code class="docutils literal"><span class="pre">strace</span></code> utility.</p>
<p>There are many strategies to resolve these kind of issues, each with it
merits. Possible solutions include:</p>
<ul class="simple">
<li>Adopting a polled mode of the system API concerned (if available).</li>
<li>Arranging for another core to perform the function and synchronizing with
that core via constructs that will not block the L-thread.</li>
<li>Affinitizing the thread to another scheduler devoted (as a matter of policy)
to handling threads wishing to make blocking calls, and then back again when
finished.</li>
</ul>
</div>
<div class="section" id="locks-and-spinlocks">
<span id="porting-locks-and-spinlocks"></span><h4>40.5.3.3. Locks and spinlocks</h4>
<p>Locks and spinlocks are another source of blocking behavior that for the same
reasons as system calls will need to be addressed.</p>
<p>If the application design ensures that the contending L-threads will always
run on the same scheduler then it its probably safe to remove locks and spin
locks completely.</p>
<p>The only exception to the above rule is if for some reason the
code performs any kind of context switch whilst holding the lock
(e.g. yield, sleep, or block on a different lock, or on a condition variable).
This will need to determined before deciding to eliminate a lock.</p>
<p>If a lock cannot be eliminated then an L-thread mutex can be substituted for
either kind of lock.</p>
<p>An L-thread blocking on an L-thread mutex will be suspended and will cause
another ready L-thread to be resumed, thus not blocking the scheduler. When
default behavior is required, it can be used as a direct replacement for a
pthread mutex lock.</p>
<p>Spin locks are typically used when lock contention is likely to be rare and
where the period during which the lock may be held is relatively short.
When the contending L-threads are running on the same scheduler then an
L-thread blocking on a spin lock will enter an infinite loop stopping the
scheduler completely (see <a class="reference internal" href="#porting-infinite-loops"><span class="std std-ref">Infinite loops</span></a> below).</p>
<p>If the application design ensures that contending L-threads will always run
on different schedulers then it might be reasonable to leave a short spin lock
that rarely experiences contention in place.</p>
<p>If after all considerations it appears that a spin lock can neither be
eliminated completely, replaced with an L-thread mutex, or left in place as
is, then an alternative is to loop on a flag, with a call to
<code class="docutils literal"><span class="pre">lthread_yield()</span></code> inside the loop (n.b. if the contending L-threads might
ever run on different schedulers the flag will need to be manipulated
atomically).</p>
<p>Spinning and yielding is the least preferred solution since it introduces
ready queue backlog (see also <a class="reference internal" href="#ready-queue-backlog"><span class="std std-ref">Ready queue backlog</span></a>).</p>
</div>
<div class="section" id="sleeps-and-delays">
<span id="porting-sleeps-and-delays"></span><h4>40.5.3.4. Sleeps and delays</h4>
<p>Yet another kind of blocking behavior (albeit momentary) are delay functions
like <code class="docutils literal"><span class="pre">sleep()</span></code>, <code class="docutils literal"><span class="pre">usleep()</span></code>, <code class="docutils literal"><span class="pre">nanosleep()</span></code> etc. All will have the
consequence of stalling the L-thread scheduler and unless the delay is very
short (e.g. a very short nanosleep) calls to these functions will need to be
eliminated.</p>
<p>The simplest mitigation strategy is to use the L-thread sleep API functions,
of which two variants exist, <code class="docutils literal"><span class="pre">lthread_sleep()</span></code> and <code class="docutils literal"><span class="pre">lthread_sleep_clks()</span></code>.
These functions start an rte_timer against the L-thread, suspend the L-thread
and cause another ready L-thread to be resumed. The suspended L-thread is
resumed when the rte_timer matures.</p>
</div>
<div class="section" id="infinite-loops">
<span id="porting-infinite-loops"></span><h4>40.5.3.5. Infinite loops</h4>
<p>Some applications have threads with loops that contain no inherent
rescheduling opportunity, and rely solely on the OS time slicing to share
the CPU. In a cooperative environment this will stop everything dead. These
kind of loops are not hard to identify, in a debug session you will find the
debugger is always stopping in the same loop.</p>
<p>The simplest solution to this kind of problem is to insert an explicit
<code class="docutils literal"><span class="pre">lthread_yield()</span></code> or <code class="docutils literal"><span class="pre">lthread_sleep()</span></code> into the loop. Another solution
might be to include the function performed by the loop into the execution path
of some other loop that does in fact yield, if this is possible.</p>
</div>
<div class="section" id="porting-thread-local-storage">
<span id="id7"></span><h4>40.5.3.6. Thread local storage</h4>
<p>If the application uses thread local storage, the use case should be
studied carefully.</p>
<p>In a legacy pthread application either or both the <code class="docutils literal"><span class="pre">__thread</span></code> prefix, or the
pthread set/get specific APIs may have been used to define storage local to a
pthread.</p>
<p>In some applications it may be a reasonable assumption that the data could
or in fact most likely should be placed in L-thread local storage.</p>
<p>If the application (like many DPDK applications) has assumed a certain
relationship between a pthread and the CPU to which it is affinitized, there
is a risk that thread local storage may have been used to save some data items
that are correctly logically associated with the CPU, and others items which
relate to application context for the thread. Only a good understanding of the
application will reveal such cases.</p>
<p>If the application requires an that an L-thread is to be able to move between
schedulers then care should be taken to separate these kinds of data, into per
lcore, and per L-thread storage. In this way a migrating thread will bring with
it the local data it needs, and pick up the new logical core specific values
from pthread local storage at its new home.</p>
</div>
</div>
<div class="section" id="pthread-shim">
<span id="id8"></span><h3>40.5.4. Pthread shim</h3>
<p>A convenient way to get something working with legacy code can be to use a
shim that adapts pthread API calls to the corresponding L-thread ones.
This approach will not mitigate any of the porting considerations mentioned
in the previous sections, but it will reduce the amount of code churn that
would otherwise been involved. It is a reasonable approach to evaluate
L-threads, before investing effort in porting to the native L-thread APIs.</p>
<div class="section" id="id9">
<h4>40.5.4.1. Overview</h4>
<p>The L-thread subsystem includes an example pthread shim. This is a partial
implementation but does contain the API stubs needed to get basic applications
running. There is a simple &#8220;hello world&#8221; application that demonstrates the
use of the pthread shim.</p>
<p>A subtlety of working with a shim is that the application will still need
to make use of the genuine pthread library functions, at the very least in
order to create the EAL threads in which the L-thread schedulers will run.
This is the case with DPDK initialization, and exit.</p>
<p>To deal with the initialization and shutdown scenarios, the shim is capable of
switching on or off its adaptor functionality, an application can control this
behavior by the calling the function <code class="docutils literal"><span class="pre">pt_override_set()</span></code>. The default state
is disabled.</p>
<p>The pthread shim uses the dynamic linker loader and saves the loaded addresses
of the genuine pthread API functions in an internal table, when the shim
functionality is enabled it performs the adaptor function, when disabled it
invokes the genuine pthread function.</p>
<p>The function <code class="docutils literal"><span class="pre">pthread_exit()</span></code> has additional special handling. The standard
system header file pthread.h declares <code class="docutils literal"><span class="pre">pthread_exit()</span></code> with
<code class="docutils literal"><span class="pre">__attribute__((noreturn))</span></code> this is an optimization that is possible because
the pthread is terminating and this enables the compiler to omit the normal
handling of stack and protection of registers since the function is not
expected to return, and in fact the thread is being destroyed. These
optimizations are applied in both the callee and the caller of the
<code class="docutils literal"><span class="pre">pthread_exit()</span></code> function.</p>
<p>In our cooperative scheduling environment this behavior is inadmissible. The
pthread is the L-thread scheduler thread, and, although an L-thread is
terminating, there must be a return to the scheduler in order that the system
can continue to run. Further, returning from a function with attribute
<code class="docutils literal"><span class="pre">noreturn</span></code> is invalid and may result in undefined behavior.</p>
<p>The solution is to redefine the <code class="docutils literal"><span class="pre">pthread_exit</span></code> function with a macro,
causing it to be mapped to a stub function in the shim that does not have the
<code class="docutils literal"><span class="pre">noreturn</span></code> attribute. This macro is defined in the file
<code class="docutils literal"><span class="pre">pthread_shim.h</span></code>. The stub function is otherwise no different than any of
the other stub functions in the shim, and will switch between the real
<code class="docutils literal"><span class="pre">pthread_exit()</span></code> function or the <code class="docutils literal"><span class="pre">lthread_exit()</span></code> function as
required. The only difference is that the mapping to the stub by macro
substitution.</p>
<p>A consequence of this is that the file <code class="docutils literal"><span class="pre">pthread_shim.h</span></code> must be included in
legacy code wishing to make use of the shim. It also means that dynamic
linkage of a pre-compiled binary that did not include pthread_shim.h is not be
supported.</p>
<p>Given the requirements for porting legacy code outlined in
<a class="reference internal" href="#porting-legacy-code-to-run-on-lthreads"><span class="std std-ref">Porting legacy code to run on L-threads</span></a> most applications will require at
least some minimal adjustment and recompilation to run on L-threads so
pre-compiled binaries are unlikely to be met in practice.</p>
<p>In summary the shim approach adds some overhead but can be a useful tool to help
establish the feasibility of a code reuse project. It is also a fairly
straightforward task to extend the shim if necessary.</p>
<p><strong>Note:</strong> Bearing in mind the preceding discussions about the impact of making
blocking calls then switching the shim in and out on the fly to invoke any
pthread API this might block is something that should typically be avoided.</p>
</div>
<div class="section" id="building-and-running-the-pthread-shim">
<h4>40.5.4.2. Building and running the pthread shim</h4>
<p>The shim example application is located in the sample application
in the performance-thread folder</p>
<p>To build and run the pthread shim example</p>
<ol class="arabic">
<li><p class="first">Go to the example applications folder</p>
<div class="highlight-console"><div class="highlight"><pre><span></span><span class="go">export RTE_SDK=/path/to/rte_sdk</span>
<span class="go">cd ${RTE_SDK}/examples/performance-thread/pthread_shim</span>
</pre></div>
</div>
</li>
<li><p class="first">Set the target (a default target is used if not specified). For example:</p>
<div class="highlight-console"><div class="highlight"><pre><span></span><span class="go">export RTE_TARGET=x86_64-native-linuxapp-gcc</span>
</pre></div>
</div>
<p>See the DPDK Getting Started Guide for possible RTE_TARGET values.</p>
</li>
<li><p class="first">Build the application:</p>
<div class="highlight-console"><div class="highlight"><pre><span></span><span class="go">make</span>
</pre></div>
</div>
</li>
<li><p class="first">To run the pthread_shim example</p>
<div class="highlight-console"><div class="highlight"><pre><span></span><span class="go">lthread-pthread-shim -c core_mask -n number_of_channels</span>
</pre></div>
</div>
</li>
</ol>
</div>
</div>
<div class="section" id="l-thread-diagnostics">
<span id="lthread-diagnostics"></span><h3>40.5.5. L-thread Diagnostics</h3>
<p>When debugging you must take account of the fact that the L-threads are run in
a single pthread. The current scheduler is defined by
<code class="docutils literal"><span class="pre">RTE_PER_LCORE(this_sched)</span></code>, and the current lthread is stored at
<code class="docutils literal"><span class="pre">RTE_PER_LCORE(this_sched)-&gt;current_lthread</span></code>. Thus on a breakpoint in a GDB
session the current lthread can be obtained by displaying the pthread local
variable <code class="docutils literal"><span class="pre">per_lcore_this_sched-&gt;current_lthread</span></code>.</p>
<p>Another useful diagnostic feature is the possibility to trace significant
events in the life of an L-thread, this feature is enabled by changing the
value of LTHREAD_DIAG from 0 to 1 in the file <code class="docutils literal"><span class="pre">lthread_diag_api.h</span></code>.</p>
<p>Tracing of events can be individually masked, and the mask may be programmed
at run time. An unmasked event results in a callback that provides information
about the event. The default callback simply prints trace information. The
default mask is 0 (all events off) the mask can be modified by calling the
function <code class="docutils literal"><span class="pre">lthread_diagniostic_set_mask()</span></code>.</p>
<p>It is possible register a user callback function to implement more
sophisticated diagnostic functions.
Object creation events (lthread, mutex, and condition variable) accept, and
store in the created object, a user supplied reference value returned by the
callback function.</p>
<p>The lthread reference value is passed back in all subsequent event callbacks,
the mutex and APIs are provided to retrieve the reference value from
mutexes and condition variables. This enables a user to monitor, count, or
filter for specific events, on specific objects, for example to monitor for a
specific thread signaling a specific condition variable, or to monitor
on all timer events, the possibilities and combinations are endless.</p>
<p>The callback function can be set by calling the function
<code class="docutils literal"><span class="pre">lthread_diagnostic_enable()</span></code> supplying a callback function pointer and an
event mask.</p>
<p>Setting <code class="docutils literal"><span class="pre">LTHREAD_DIAG</span></code> also enables counting of statistics about cache and
queue usage, and these statistics can be displayed by calling the function
<code class="docutils literal"><span class="pre">lthread_diag_stats_display()</span></code>. This function also performs a consistency
check on the caches and queues. The function should only be called from the
master EAL thread after all slave threads have stopped and returned to the C
main program, otherwise the consistency check will fail.</p>
</div>
</div>
</div>


           </div>
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="ipsec_secgw.html" class="btn btn-neutral float-right" title="41. IPsec Security Gateway Sample Application" accesskey="n">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="ptpclient.html" class="btn btn-neutral" title="39. PTP Client Sample Application" accesskey="p"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  

    <script type="text/javascript">
        var DOCUMENTATION_OPTIONS = {
            URL_ROOT:'../',
            VERSION:'16.04.0',
            COLLAPSE_INDEX:false,
            FILE_SUFFIX:'.html',
            HAS_SOURCE:  true
        };
    </script>
      <script type="text/javascript" src="../_static/jquery.js"></script>
      <script type="text/javascript" src="../_static/underscore.js"></script>
      <script type="text/javascript" src="../_static/doctools.js"></script>

  

  
  
    <script type="text/javascript" src="../_static/js/theme.js"></script>
  

  
  
  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.StickyNav.enable();
      });
  </script>
   

</body>
</html>